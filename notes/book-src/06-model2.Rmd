# Day 6

## Review

- What is a model?

  - Scientific vs. Thought vs. Mathematical
  
  - Statistical?
  
- Why do we use models?

<br>

- Trend analysis

  - What's happening with total kjeldahl nitrogen in Kansas?

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(gamair)
library(nlme)
library(mgcv)
library(tidyverse)
url = "https://raw.githubusercontent.com/rmshksu/teaching-data/4c51b2017bb69c11d7ee918cac7d594c07b39549/cruisedata.csv"
cruise = read.csv(url, stringsAsFactors = FALSE)
url = "https://raw.githubusercontent.com/rmshksu/teaching-data/4c51b2017bb69c11d7ee918cac7d594c07b39549/kslr_tkn.csv"
kslr_tkn = read.csv(url, stringsAsFactors = FALSE)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center'}
plot(kslr_tkn$Year, kslr_tkn$UOM, 
     col="#00000060",
     xlab = "Year",
     ylab = "TKN (mg/L)")
```

- Simple linear regression

\[
y_i = \beta_0 + \beta_1 t_i + \epsilon_i
\]

\[
\epsilon_i \sim N(0, \sigma^2 \textbf{I})
\]

<br>

\[
y_i \sim N(\mu,\sigma^2 \textbf{I})
\]

\[
\mu = \beta_0 + \beta_1 t_i
\]

```{r}
mod_df = data.frame(UOM = kslr_tkn$UOM,
                    Year = kslr_tkn$Year,
                    huc08 = kslr_tkn$huc08)

m1 = lm(UOM ~ Year, data = mod_df)
summary(m1)
```

```{r,fig.align='center'}
or = order(mod_df$Year)
plot(mod_df$Year, mod_df$UOM, 
     col="#D1D1D180",
     xlab = "Year",
     ylab = "TKN (mg/L)")
lines(mod_df$Year[or], predict(m1, type="response")[or], lwd = 2, col = "#512885")
```

<br>

- What about space?

  - Add a "cluster effect"
  
\[
y_{ij} = \beta_0 + \beta_1 t_i + \beta_2 s_j \epsilon_{ij}
\]

\[
\epsilon_{ij} \sim N(0, \sigma^2 \textbf{I})
\]

- $s_j$ pertains to the "HUC" levels

  - Think of it as a categorical predictor
  
  - Goes to $0$ based on which HUC is present

```{r}
m2 = lm(UOM ~ Year + as.factor(huc08), data = mod_df)
summary(m2)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center'}
xg = seq(min(mod_df$Year, na.rm = TRUE),
         max(mod_df$Year, na.rm = TRUE),
         length.out = 200)
levs = levels(model.frame(m2)[["as.factor(huc08)"]])
cols = seq_along(levs)

plot(mod_df$Year, mod_df$UOM, 
     col="#D1D1D180",
     xlab = "Year",
     ylab = "TKN (mg/L)")
for (i in 1:length(levs)) {
  lv = levs[i]
  nd = data.frame(Year = xg, huc08 = factor(lv, levels = levs))
  lines(xg, predict(m2, nd), lwd = 1, col = cols[i])
}
legend("topright", legend = levs, lwd = 1, col = cols, bty = "n")
```

- Where are we headed?

<br>

## Stringing sentences

- Correlative inference

  - Which cruise liner should you avoid?

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center'}
par(mar = c(5, 9, 4, 2))
boxplot(cruise$infected_proportion ~ cruise$liner,
        xlab = "Proportion infected",
        ylab = "",
        main = "Norovirus outbreaks on cruise liners",
        horizontal = TRUE,
        las = 1,
        cex.axis = 0.7,
        col = "#D1D1D190")
```

\[
\left[y|\alpha, \beta \right] \equiv \text{Beta}(\alpha,\beta)
\]

\[
\alpha = \mu \phi \qquad \beta = (1 - \mu) \phi \qquad g(\mu) = \textbf{X}^\prime \boldsymbol{\beta}
\]

```{r}
m1 = gam(infected_proportion ~ as.factor(liner) - 1,
         data = cruise, family = betar(link = "logit"))
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center'}
# liner levels exactly as used in the fitted model
levs <- levels(model.frame(m1)[["as.factor(liner)"]])

# prediction data: one row per liner
nd <- data.frame(liner = factor(levs, levels = levs))

# get fitted linear predictor + SE per liner
pr <- predict(m1, newdata = nd, type = "link", se.fit = TRUE)

# 95% CI on link scale, then transform to response
invlogit <- m1$family$linkinv
fit_r <- invlogit(pr$fit)
lo_r  <- invlogit(pr$fit - 1.96 * pr$se.fit)
hi_r  <- invlogit(pr$fit + 1.96 * pr$se.fit)

# plot CI segments only, labeled by the original liner names
op <- par(mar = c(5, 10, 4, 2))

plot(
  NULL,
  xlim = range(c(lo_r, hi_r)),
  ylim = c(0.5, length(levs) + 0.5),
  yaxt = "n",
  xlab = "Predicted infected proportion (95% CI)",
  ylab = "",
  main = "Predicted mean infection by liner"
)

segments(lo_r, seq_along(levs), hi_r, seq_along(levs), lwd = 3)

axis(2, at = seq_along(levs), labels = levs, las = 1, cex.axis = 0.75)

par(op)
```

- Seems like Fred Olsen is a bad idea...

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center'}
freq_df = as.data.frame(table(cruise$liner))
freq_df = data.frame("Liner" = freq_df$Var1[1:10],
                     "Frequency" = freq_df$Freq[1:10])
card_kable(data = freq_df,
           title = "Frequency distribution of cruise liners",
           subtitle = "Half of it, that is...",
           width = "50%",
           header_color = "#51288590")

url = "https://raw.githubusercontent.com/rmshksu/teaching-data/4c51b2017bb69c11d7ee918cac7d594c07b39549/bbreg_samp.csv"
samp = read.csv(url)
samp = samp[,-1]
```

<br>

## Telling stories

- What just happened?

- Are these results a problem? 

  - Is the context behind them bad?
  
- What is data?

- What is a statistic?

  - What is it for?
  
  - What are the limitations of statistics? Of science?
  
<br>

- A major step in telling a story

  - Having an idea for the plot
  
- What makes a good plot?

<br>

- How do scientific inquiries happen?

  - By accident (Grokking)
  
  - By madness (Game theory)
  
  - Taking risks (Chemotherapy)
  
  - Via spite (Scale-free networks)
  
  - "Things breaking" (?)
  
<br>

- Is Fred Olsen a potent vector of norovirus?

  - Are Carnival cruises really that *safe* of an option?

```{r , eval=FALSE, fig.align='center', message=FALSE, warning=FALSE}
bbreg = function(){
  
  # process model
  for(i in 1:n){
    y[i] ~ dbeta(alpha[i], beta[i])
    
    # mean/precision parameterization
    alpha[i] <- mu[i] * phi
    beta[i]  <- (1-mu[i]) * phi
    
    # logit link function
    logit(mu[i]) <- a + b*x[i]
  }
  
  # priors
  phi ~ dgamma(1,.01)
  a ~ dnorm(0,.1)
  b ~ dnorm(0,.1)
  
}

# setting up jags data
jags_dat = list(y = cruise$infected_proportion,
                x = as.integer(factor(cruise$liner)),
                n = nrow(cruise))

# initial proposal values
inits = list(a = .01, b = .01, phi = 1)

# fitting the model
bbreg_mcmc = jags.fit(jags_dat,c("a","b","phi"),
                      bbreg,inits,n.adapt=5000,
                      n.update=5000,n.iter=100000,
                      thin=1,n.chains=1)
```

```{r , warning=FALSE,message=FALSE,fig.align='center'}
# reproducibility seed
set.seed(73)

# number of posterior samples
K = nrow(samp)

# set to carnival and fred olsen
x_pred = c(3,8)
a_post = samp[,1] # intercept post
b_post = samp[,2] # effect par post
phi_post = samp[,3] # precision par post

# vectorized posterior logit(mu) predictions
eta_post = outer(a_post, x_pred,
                 FUN = function(ai, x) ai) + 
  outer(b_post, x_pred)

# "un-do" the link function
mu_post = invlogit(eta_post)

# alpha posterior predictions
alpha_post = mu_post * phi_post

# beta posterior predictions
beta_post = (1 - mu_post) * phi_post

# posterior predictions of y
y_post = matrix(rbeta(length(alpha_post),
                      alpha_post, beta_post),
                nrow = K, ncol = length(x_pred))
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center'}
hist(y_post[,1],
     xlab = "Proportion infected",
     main = "Posterior prediction of Carnival outbreaks",
     col = "white")
hist(y_post[,2],
     xlab = "Proportion infected",
     main = "Posterior prediction of Fred Olsen outbreaks",
     col = "white")
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center'}
cred_int = t(apply(y_post, 2, quantile, probs = c(0.025, 0.5, 0.975)))

op = par(mar = c(5, 10, 4, 2))

plot(
  NULL,
  xlim = range(c(lo_r, hi_r)),
  ylim = c(0.5, length(levs) + 0.5),
  yaxt = "n",
  xlab = "Predicted infected proportion (95% CI)",
  ylab = "",
  main = "Predicted mean infection by liner"
)

segments(lo_r, seq_along(levs), hi_r, seq_along(levs), lwd = 3)
segments(cred_int[1,1], 3, cred_int[1,3], 3, lwd = 4, col = "green4")
segments(cred_int[2,1], 8, cred_int[2,3], 8, lwd = 4, col = "green4")
points(cred_int[1,2], 3, cex = 2, pch = "*")
points(cred_int[2,2], 8, cex = 2, pch = "*")

axis(2, at = seq_along(levs), labels = levs, las = 1, cex.axis = 0.75)

par(op)
```

<br>

**Any questions?**

<br>

## Linear models

- We learned the alphabet
 
  - Developed our vocabulary
  
  - And by the end of STAT 240, we formed sentences
  
- But do we know what those sentences meant?

  - How can we develop a story if we're just throwing things together?
  
  - We need to understand the *logic* of our phrases
  
  - The best way to do that is to look at something with *well-defined* logic
  
<br>

- In a writing course:

  - Of Mice and Men
  
  - To Kill a Mockingbird
  
  - Frankenstein
  
- Theater

  - Oedipus
  
  - Hamlet
  
  - MacBeth
  
- History

  - Fall of the Roman Empire
  
  - Holy Crusades
  
  - Transition between WW1 to WW2
  
<br>

- For statistics, we use linear models

\[
\boldsymbol{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_k x_k + \boldsymbol{\epsilon}
\]

\[
\boldsymbol{y} = \textbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}
\]

- Why? 

  - We know (almost) everything about them
  
  - Easy to "extend" the theory to other models
  
<br>

- "Linear" has a different definition here

  - We'll stick to "linear in the parameters"
  
  - Supplemental material will include a more *robust* definition
  
\[
\begin{aligned}
& \boldsymbol{y} = \beta_0 + \beta_1 \boldsymbol{x}_1 + \beta_2 \boldsymbol{x}_1^2 + \boldsymbol{\epsilon} \\
\\
& \boldsymbol{y} =  \beta_0 + \beta_1 10^x + \boldsymbol{\epsilon} \\
\\
& \boldsymbol{y} = \beta_0 + \beta_1 g(\boldsymbol{x}_1) + \beta_2 \log(\boldsymbol{x}_2) + \boldsymbol{\epsilon}\\
\\
& \boldsymbol{y} = \beta_0 + e^{\beta_1 \boldsymbol{x_1}} + \boldsymbol{\epsilon}\\
\end{aligned}
\]

- Which one of these models is linear?

- What's a common nonlinear model?

\[
P(t) = P_0 e^{rt}
\]

<br>

## Parameter estimation

- Estimating $\boldsymbol{\beta}$ 

\[
(\boldsymbol{X}^\prime \boldsymbol{X})^{-1} \boldsymbol{X}^\prime \boldsymbol{y}
\]

<br>

\[
\underset{i}{\arg\min} \sum_{i=1}^n (\boldsymbol{y}_i - \boldsymbol{x}_i^\prime \boldsymbol{\beta})^2
\]

<br>

\[
\underset{i}{\arg\max} \mathcal{L}(\boldsymbol{y}|\boldsymbol{\beta},\sigma^2)
\]

<br>

- In R

```{r}
url = "https://raw.githubusercontent.com/rmshksu/teaching-data/refs/heads/main/heart.csv"
heart = read.csv(url) # heart biometrics data
y = heart$chol # serum cholesterol
x1 = heart$age # subject age
```

- Ordinary least squares

```{r}
X = cbind(1,x1) # design matrix
# matrix form of ordinary least squares
bhat = solve(t(X)%*%X)%*%t(X)%*%y
bhat
```

<br>

- Optimization

```{r}
# nelder mead optimization of least squares
optim(par=c(0,0),method = c("Nelder-Mead"),fn=function(beta){sum((y-(beta[1]+beta[2]*x1))^2)})
```

<br>

- Using R's `lm()` method

```{r}
# linear model in R
lm(y ~ x1)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center'}
par(mar = c(4.5,4.5,1,1))
plot(x1,y, type = "n",
     xlab = "Age",
     ylab = "Serum Cholesteorol")
abline(v = seq(30,70,10), col = "#d1d1d190")
abline(h = seq(200,500,100), col = "#d1d1d190")
points(x1,y, pch = 20, cex = 1.2, col = "#00000080")
abline(a = bhat[1], b = bhat[2], col = "gold", lwd = 3)
```

<br>

- Maximum Likelihood Estimation

```{r}
# negative log-likelihood of normal model
nll = function(par){
beta = par[1:2]
sig2 = par[3]
-sum(dnorm(y,X%*%beta,sqrt(sig2),log=TRUE))
}
optim(par=c(0,0,2),fn=nll,method = "Nelder-Mead")
```

<br>

- `lm()`

```{r}
# all basic summaries of R linear model
summary(lm(y ~ x1))
```

<br>

## Live example

- R code

<br>

## The path forward

- Linear model theory and application

- Hypothesis testing of parameters

- Building models

- Assignment 2

- Assignment 3

## Go away
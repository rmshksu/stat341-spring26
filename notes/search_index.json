[["day-1.html", "Biometrics II - STAT 341 1 Day 1 1.1 Welcome + Mandatory Formalities 1.2 Assignments 1.3 Final Project 1.4 Review 1.5 Assignment 1 1.6 Go away", " Biometrics II - STAT 341 R. M. Sholl Spring 2026 1 Day 1 1.1 Welcome + Mandatory Formalities About me Course website Canvas Email, don’t Canvas message Syllabus What’s this course about? Is it right for you? Statistical programming Reproducibility and academic honesty How I teach Grades Content Schedule All dates are subject to change. Week Topics Assignments 1 Review and math overview HW 1 / Journal 1 2 Math and R programming Journal 2 3 Statistical modeling HW 2 / Journal 3 4 Linear models I Journal 4 5 Linear models II HW 3 / Journal 5 6 Hypothesis testing Journal 6 7 Prediction HW 4 / Journal 7 8 Inference Project proposal / Journal 8 9 SPRING BREAK 10 Model checking Literature review 11 Review and catch-up Exploratory analysis 12 Assumptions and errors Model descriptions 13 Generalized linear models Model fitting/checking 14 Mixed Models Results and inference 15 Class choice I Rough draft 16 Class choice II Presentation 17 Finals Final Project + Report About you! 1.2 Assignments Project-based education Group work/collaboration Structure of assignments: Due every other week* (2 person) Group assignments* Different partners each time *Except this week Purpose of assignments: Build comfort with R programming Practice essential methods “Interview” potential project partners 1.3 Final Project 2-3 person groups Working alone ([softly] Don’t.) Propose a project (Week 8) Academic research Industry solutions General interest Final presentation (Dead &amp; Finals week) Final paper (Friday of finals) 1.4 Review 1.4.1 General Intro Stats Everyone should know these Regardless of who your intro stats instructor was Mean and standard deviation \\[ \\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i \\qquad\\qquad s = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}(x_i - \\bar{x})^2} \\] # base R dataset on fiji earthquakes quakes = datasets::quakes # mean magnitude mean(quakes$mag) ## [1] 4.6204 # variance of magnitude var(quakes$mag) ## [1] 0.1622261 # standard deviation of magnitude sd(quakes$mag) ## [1] 0.402773 Quantiles/Quartiles \\[ \\begin{array}{|c|c|c|c|c|} \\hline \\text{Min} &amp; \\text{Q}_1 &amp; \\text{Median} &amp; \\text{Q}_3 &amp; \\text{Max} \\\\ \\hline 0^{th} &amp; 25^{th} &amp; 50^{th} &amp; 75^{th} &amp; 100^{th} \\\\ \\hline \\end{array} \\] # five number summary as quantiles quantile(quakes$mag, c(0,0.25,0.5,0.75,1)) ## 0% 25% 50% 75% 100% ## 4.0 4.3 4.6 4.9 6.4 # five number summary base R function fivenum(quakes$mag) ## [1] 4.0 4.3 4.6 4.9 6.4 Histograms and Boxplots # base R histogram hist(quakes$mag, xlab = &quot;Magnitude&quot;, # x axis label main = &quot;Earthquakes off Fiji&quot;, # title col = &quot;white&quot;) # bar color hist(quakes$mag, # data xlab = &quot;Magnitude&quot;, main = &quot;Earthquakes off Fiji&quot;, col = &quot;white&quot;) abline(v = mean(quakes$mag), # vertical mean line lty = 2, # line type (dashed) lwd = 2, # line width col = &quot;gold&quot;) # line color # margin control par(mar = c(3,1,1,1)) # bottom, left, top, right # base R boxplot boxplot(quakes$mag, horizontal = T, # set as horizontal (T = TRUE) col = &quot;white&quot;) # central box color par(mar = c(3,1,1,1)) boxplot(quakes$mag, horizontal = T, col = &quot;white&quot;) abline(v = c(4,4.3,4.6,4.9,5.7), # vertical quantile lines lty = 2, lwd = 2, # color palette col = c(&quot;#FED976&quot;,&quot;#FEB24C&quot;,&quot;#FD8D3C&quot;,&quot;#E31A1C&quot;,&quot;#800026&quot;)) text(c(4.1,4.24,4.5,4.82,5.5), # x position c(0.9,1.15,0.9,1.15,0.9), # y position c(&quot;Min&quot;,&quot;Q1&quot;,&quot;Med&quot;,&quot;Q3&quot;,&quot;Max&quot;), # quantile labels col = c(&quot;#FED976&quot;,&quot;#FEB24C&quot;,&quot;#FD8D3C&quot;,&quot;#E31A1C&quot;,&quot;#800026&quot;)) \\(z\\)-scores \\[ z = \\frac{x - \\bar{x}}{s} \\] # transform the entire dataset into z-scores z = (quakes$mag - mean(quakes$mag))/sd(quakes$mag) Normal distributions \\[ X \\sim N(\\mu,\\sigma^2) \\qquad\\qquad \\frac{X - \\mu}{\\sigma} = Z \\sim N(0,1) \\] par(mar = c(4.5,4.5,1,1)) curve(dnorm(x), # standard normal density xlim = c(-4,4), # x axis limits lwd = 2, xlab = &quot;X&quot;, ylab = &quot;P(X=x)&quot;) # y axis label # z score for a 5.5 magnitude earthquake z_score = (5.5 - mean(quakes$mag))/sd(quakes$mag) 1 - pnorm(z_score) # 1 - z probability (same as z-table) ## [1] 0.01448625 Scatterplots # base R scatterplot plot(quakes$stations,quakes$mag, # x then y xlab = &quot;Number of stations reporting&quot;, # x axis label ylab = &quot;Magnitude&quot;, # y axis label main = &quot;Earthquakes off Fiji&quot;, # title pch = 20, # point shape col = &quot;#00000080&quot;) # point color Least squares regression \\[ y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\] # predictor x = quakes$stations # response y = quakes$mag # base R linear model m = lm(y ~ x) coef(m) # coefficients ## (Intercept) x ## 4.09726756 0.01565421 plot(quakes$stations,quakes$mag, xlab = &quot;Number of stations reporting&quot;, ylab = &quot;Magnitude&quot;, main = &quot;Earthquakes off Fiji&quot;, pch = 20, col = &quot;#00000080&quot;) # add regression line lines(sort(quakes$stations),sort(m$fitted.values), lwd = 2, col = &quot;gold&quot;) # line color Confidence intervals # 95% confidence interval # mean magnitude xbar_mag = mean(quakes$mag) # standard deviation of magnitude s_mag = sd(quakes$mag) # sample size n_mag = as.numeric(nrow(quakes)) # margin of error moe_mag = 1.96*(s_mag/sqrt(n_mag)) xbar_mag - moe_mag # lower bound ## [1] 4.595436 xbar_mag # point estimate ## [1] 4.6204 xbar_mag + moe_mag # upper bound ## [1] 4.645364 Hypothesis tests # schizophrenic reaction time data schiz = data.frame(time = scan(&quot;data/schiz.txt&quot;,skip=5), schizophrenia = c(rep(&quot;no&quot;,30*11),rep(&quot;yes&quot;,30*6))) library(dplyr) # R data wrangling package sstats = schiz %&gt;% # group by yes/no schiz group_by(schizophrenia) %&gt;% # mean/var/sample size for yes/no strata summarise(xbar = mean(time), s2 = var(time), n = n()) # print dataframe sstats ## # A tibble: 2 × 4 ## schizophrenia xbar s2 n ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 no 310. 4209. 330 ## 2 yes 507. 69089. 180 \\[ \\begin{aligned} \\text{H}_0: \\mu_1 - \\mu_2 = 0 \\\\ \\\\ \\text{H}_a: \\mu_1 - \\mu_2 &lt; 0 \\end{aligned} \\] \\[ t^* = \\frac{(\\bar{x}_1 - \\bar{x}_2) + (\\mu_1 - \\mu_2)}{\\sqrt{(s_1^2/n_1) + (s_2^2/n_2)}} \\] # test statistics for independent difference in means tstar = as.numeric((sstats[1,2] - sstats[2,2])/ sqrt((sstats[1,3]/sstats[1,4]) + (sstats[2,3]/sstats[2,4]))) tstar ## [1] -9.877136 # p-vale for t-pivot test statistic pt(tstar,min(sstats[,4])-1) ## [1] 6.114281e-19 1.4.2 Biometrics I Review for my students Potentially everyone It’s okay to not know these Come to office hours though Discrete expected value \\[ \\mathbb{E}X = \\sum_x x P(X=x) \\] Binomial distribution Discrete, binary processes Success/failure \\[ X \\sim \\text{Bin}(n,p) \\qquad \\qquad \\mathbb{E}X = np \\qquad \\qquad \\mathbb{V}X = np(1-p) \\] Poisson distribution Positive discrete counts \\[ X \\sim \\text{Pois}(\\lambda) \\qquad \\qquad \\mathbb{E}X = \\mathbb{V}X = \\lambda \\] Geometric distribution Number of trials to reach success \\[ X \\sim \\text{Geom}(p) \\qquad \\qquad \\mathbb{E}X = 1/p \\] Hypergeometric distribution Geometric distribution Sampling without replacement \\[ X \\sim \\text{Hyper}(N,K,n) \\qquad \\qquad \\mathbb{E}X = n \\frac{K}{N} \\] Continuous expected values \\[ \\mathbb{E}X = \\int_{\\chi}x f_x(x) \\] Gamma distribution Positive continuous values \\[ X \\sim \\text{Gamma}(\\alpha,\\theta) \\qquad \\qquad \\mathbb{E}X = \\alpha \\theta \\qquad \\qquad \\mathbb{V}X = \\alpha \\theta^2 \\] Beta distribution Proportions/probabilities \\[ X \\sim \\text{Beta}(\\alpha,\\beta) \\qquad \\qquad \\mathbb{E}X = \\frac{\\alpha}{\\alpha + \\beta} \\qquad \\qquad \\mathbb{V}X = \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)} \\] Uniform distribution Equi-probable events Defined boundaries (a and b) \\[ X \\sim \\text{Unif}(a,b) \\qquad \\qquad \\mathbb{E}X = \\frac{1}{2}(b-a) \\qquad \\qquad \\mathbb{V}X = \\frac{1}{12}(b-a)^2 \\] 1.5 Assignment 1 1.6 Go away "],["day-2.html", "2 Day 2 2.1 Math overview 2.2 Matrix algebra 2.3 Calculus 2.4 Advanced Math 2.5 Next class 2.6 Assignment 1 2.7 Go away", " 2 Day 2 2.1 Math overview Not a “math class” Applied statistics class Math is a tool So we need to understand it Today only covers the essentials Supplemental material If you want it Not required (but recommended) Final project can be math focused 2.2 Matrix algebra \\[ \\text{Matrix: } \\textbf{A} = \\begin{bmatrix}1 &amp; 2 \\\\ 3 &amp; 4\\end{bmatrix} \\qquad\\qquad \\text{Vector: } \\boldsymbol{x} = \\begin{bmatrix}1 \\\\ 2 \\\\ 3\\end{bmatrix} \\qquad\\qquad \\text{Scalar: } a = 5 \\] # 2x2 matrix A = matrix(c(1,2,3,4), nrow = 2, ncol = 2, byrow = TRUE) # 1x3 vector x = c(1,2,3) # scalar (1x1 matrix/vector) a = 5 A ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 x ## [1] 1 2 3 a ## [1] 5 Sums \\[ \\textbf{A} = \\begin{bmatrix}1 &amp; 2 \\\\ 3 &amp; 4\\end{bmatrix} \\qquad\\qquad \\textbf{B} = \\begin{bmatrix}1 &amp; 1 \\\\ 1 &amp; 0\\end{bmatrix} \\] \\[ \\textbf{A} + \\textbf{B} = \\begin{bmatrix}1 &amp; 2 \\\\ 3 &amp; 4\\end{bmatrix} + \\begin{bmatrix}1 &amp; 0 \\\\ 1 &amp; 0\\end{bmatrix} = \\begin{bmatrix}2 &amp; 2 \\\\ 4 &amp; 4\\end{bmatrix} \\] B = matrix(c(1,1,1,0),2,2,T) # adding two matrices A + B ## [,1] [,2] ## [1,] 2 3 ## [2,] 4 4 # scalar + matrix a + A ## [,1] [,2] ## [1,] 6 7 ## [2,] 8 9 # scalar + vector a + x ## [1] 6 7 8 Scalar Products of Vector/Matrix \\[ 3 \\times \\begin{bmatrix}1 &amp; 2 \\\\ 3 &amp; 4\\end{bmatrix} = \\begin{bmatrix}3 &amp; 6 \\\\ 9 &amp; 12\\end{bmatrix} \\qquad\\qquad 10 \\times \\begin{bmatrix}1 \\\\ 2 \\\\ 3\\end{bmatrix} = \\begin{bmatrix}10 \\\\ 20 \\\\ 30\\end{bmatrix} \\] # scalar * matrix 3*A ## [,1] [,2] ## [1,] 3 6 ## [2,] 9 12 # scalar * vector 10*x ## [1] 10 20 30 Transpose \\[ \\textbf{A} = \\begin{bmatrix}1 &amp; 2 \\\\ 3 &amp; 4 \\\\ 5 &amp; 6 \\end{bmatrix} \\qquad\\qquad \\textbf{A}^\\prime = \\begin{bmatrix}1 &amp; 3 &amp; 5\\\\ 2 &amp; 4 &amp; 6 \\end{bmatrix} \\] A = matrix(c(1,2,3,4,5,6),3,2,T) A # 2x3 matrix ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 ## [3,] 5 6 t(A) # 3x2 when transposed ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 Matrix Products \\[ \\begin{bmatrix} 1 &amp; 3 &amp; 5 \\\\ 2 &amp; 4 &amp; 6 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\\\ 5 &amp; 6 \\end{bmatrix} = \\begin{bmatrix} (1 \\cdot 1) + (3 \\cdot 3) + (5 \\cdot 5) &amp; (1 \\cdot 2) + (3 \\cdot 4) + (5 \\cdot 6) \\\\ (2 \\cdot 1) + (4 \\cdot 3) + (6 \\cdot 5) &amp; (2 \\cdot 2) + (4 \\cdot 4) + (6 \\cdot 6) \\end{bmatrix} = \\begin{bmatrix} 35 &amp; 44 \\\\ 44 &amp; 56 \\end{bmatrix} \\] # inner product (dimension reducing) t(A)%*%A ## [,1] [,2] ## [1,] 35 44 ## [2,] 44 56 # out product (dimension increasing) A%*%t(A) ## [,1] [,2] [,3] ## [1,] 5 11 17 ## [2,] 11 25 39 ## [3,] 17 39 61 Determinants \\[ \\textbf{A} = \\begin{bmatrix} 0 &amp; -2 \\\\ 2 &amp; 0 \\end{bmatrix} \\] \\[ \\text{det}(\\textbf{A}) = \\text{det} \\left(\\begin{bmatrix} 0 &amp; -2 \\\\ 2 &amp; 0 \\end{bmatrix} \\right) = ad - bc = (0 \\times 0) - (-2 \\times 2) = 4 \\] A = matrix(c(0,-2,2,0),2,2,T) A ## [,1] [,2] ## [1,] 0 -2 ## [2,] 2 0 # determinant det(A) ## [1] 4 Matrix inverse \\[ \\textbf{A} = \\begin{bmatrix}0 &amp; -1 \\\\ 1 &amp; 0\\end{bmatrix} \\qquad\\qquad \\textbf{A}^{-1} = \\begin{bmatrix}0 &amp; 1 \\\\ -1 &amp; 0\\end{bmatrix} \\] \\[ \\textbf{AA}^{-1} = \\begin{bmatrix}0 &amp; -1 \\\\ 1 &amp; 0\\end{bmatrix}\\begin{bmatrix}0 &amp; 1 \\\\ -1 &amp; 0\\end{bmatrix} = \\begin{bmatrix}1 &amp; 0 \\\\ 0 &amp; 1\\end{bmatrix} = \\textbf{I} \\] A = matrix(c(0,-1,1,0),2,2,T) A ## [,1] [,2] ## [1,] 0 -1 ## [2,] 1 0 # inverse of A solve(A) ## [,1] [,2] ## [1,] 0 1 ## [2,] -1 0 # A * inv(A) = identity A%*%solve(A) ## [,1] [,2] ## [1,] 1 0 ## [2,] 0 1 Special matrices # identity matrix A%*%solve(A) ## [,1] [,2] ## [1,] 1 0 ## [2,] 0 1 # acts as a matrix equivalent to scalar 1 A%*%A%*%solve(A) ## [,1] [,2] ## [1,] 0 -1 ## [2,] 1 0 # zero matrix O = matrix(c(0,0,0,0),2,2,T) O%*%B ## [,1] [,2] ## [1,] 0 0 ## [2,] 0 0 # diagonal matrix `diag&lt;-`(O,c(1,2)) ## [,1] [,2] ## [1,] 1 0 ## [2,] 0 2 diag(1,2,2,T) ## [,1] [,2] ## [1,] 1 0 ## [2,] 0 1 # J matrix J = matrix(1,2,2,T) J ## [,1] [,2] ## [1,] 1 1 ## [2,] 1 1 # useful for expansions J*a ## [,1] [,2] ## [1,] 5 5 ## [2,] 5 5 # cdc vsp data cruise = read.csv(&quot;data/cruisedata.csv&quot;) x1 = cruise$infected # total infected w/ norovirus x2 = cruise$total_population # total on ship X = cbind(1,x1,x2) # design matrix n = nrow(cruise) # sample size J = matrix(1,1,n,T) # 1xn J &quot;matrix&quot; # matrix form of mean (1/n)*(J%*%X) ## x1 x2 ## [1,] 1 128.0794 2823.905 mean(x1) ## [1] 128.0794 mean(x2) ## [1] 2823.905 Trace \\[ \\text{tr}(\\textbf{A}) = \\sum_{i=1}^n a_{ii} \\] A = matrix(c(1,2,3,4,5,6,7,8,9),3,3,T) A ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 # trace() won&#39;t give you this sum(diag(A)) # sum of diagonals ## [1] 15 Eigenvalues &amp; eigenvectors A = matrix(c(3,1,0,2),2,2,T) eigen(A) # spectral decomp ## eigen() decomposition ## $values ## [1] 3 2 ## ## $vectors ## [,1] [,2] ## [1,] 1 -0.7071068 ## [2,] 0 0.7071068 Why do we care? \\[ y_i = \\beta_0 + \\beta_1x_i + \\beta_2z_i + \\epsilon_i \\] \\[ \\boldsymbol{y} = \\boldsymbol{X \\beta} + \\boldsymbol{\\epsilon} \\] \\[ \\hat{\\boldsymbol{\\beta}} = (\\boldsymbol{X}^\\prime \\boldsymbol{X})^{-1} \\boldsymbol{X}^\\prime \\boldsymbol{y} \\] x1 = cruise$start_month # predictor 1 x2 = cruise$voyage_duration # predictor 2 y = cruise$infected # response X = cbind(1,x1,x2) # design matrix # matrix method of least squares solve(t(X)%*%X)%*%t(X)%*%y ## [,1] ## 179.700447 ## x1 -5.217664 ## x2 -1.889147 # base R multiple regression coef(lm(y ~ x1 + x2)) ## (Intercept) x1 x2 ## 179.700447 -5.217664 -1.889147 2.3 Calculus I can’t teach you all of calculus in 1 class Don’t need to Exposure to some concepts Limits What happens as we ‘approach’ a value in a function Without hitting it \\[ f(x) = x^2 - x + 2 \\qquad\\qquad \\lim_{x \\to 2}f(x) = 4 \\] Plug in the approaching value \\[ \\lim_{n \\to \\infty}\\left(t_{\\alpha/2} \\frac{s}{\\sqrt{n}} \\right) = t_{\\alpha/2} \\frac{s}{\\sqrt{\\infty}} = t_{\\alpha/2} \\times 0 = 0 \\] You may notice things like \\[ s = \\sqrt{\\frac{1}{n-1}(x_i - \\bar{x})^2} \\] \\[ \\lim_{n \\to \\infty}s = \\sqrt{\\frac{1}{\\infty-1}(x_i - \\bar{x})^2} = 0 \\] \\[ \\frac{0}{0} \\Rightarrow \\text{ Undefined} \\] “Speed of convergence” Derivatives \\[ f^\\prime (a) \\equiv \\lim_{h \\to 0}\\frac{f(a + h) - f(a)}{h} \\] \\[ f(x) = x^2 - 8x + 9 \\] \\[ \\begin{aligned} &amp; \\lim_{h \\to 0} \\frac{f(a + h) - f(a)}{h} = \\lim_{h \\to 0} \\frac{[(a + h)^2 - 8(a + h) + 9] - [a^2 - 8a + 9]}{h} \\\\ \\\\ &amp; = \\lim_{h \\to 0} \\frac{a^2 + 2ah + h^2 - 8a - 8h + 9 - a^2 + 8a - 9}{h} \\\\ \\\\ &amp; = \\lim_{h \\to 0} \\frac{2ah + h^2 - 8h}{h} = \\lim_{h \\to 0 } (2a + h - 8) \\\\ \\\\ &amp; = 2a - 8 \\end{aligned} \\] Power and addition rules \\[ \\frac{d}{dx} x^2 = 2x \\qquad\\qquad \\frac{d}{dx} 8x = 8 \\qquad\\qquad \\frac{d}{dx} 9 = 0 \\] \\[ \\frac{d}{dx} x^2 - 8x + 9 = 2x - 8 \\] Numerical differentiation Using limit definition of derivative \\[ f^\\prime(2) = 2(2) - 8 = -4 \\] #limit definition of derivative lim_diff = function(f, x, h = 1e-5) { # h should always be a small step size ddx = (f(x + h) - f(x)) / h return(ddx) } fx = function(x){x^2 - 8*x + 9} lim_diff(fx,2) # evaluate at x = 2 ## [1] -3.99999 Analytic methods can get nasty fast \\[ f(x) = \\frac{\\text{sin}(x^2) + e^{x - 9} + 10x + 2}{x^2} \\] 8 step derivative Mess of chain rule, product rule, quotient rule, trigonometry \\[ f^\\prime (x) = \\frac{2x \\cos\\left(x^{2}\\right) + \\mathrm{e}^{x - 9} + 10}{x^{2}} - \\frac{2 \\left(\\sin\\left(x^{2}\\right) + \\mathrm{e}^{x - 9} + 10x + 2\\right)}{x^{3}} \\] # much nastier function fx = function(x){(sin(x^2) + exp(x - 9) + 10*x + 2)/(x^2)} lim_diff(fx,2) # also eval x = 2 ## [1] -3.464408 # true derivative fprime = function(x){(((2*x)*cos(x^2)+exp(x-9)+10)/(x^2)) - ((2*((sin(x^2))+exp(x-9)+(10*x)+2))/(x^3))} fprime(2) # eval x = 2 ## [1] -3.464443 Optimization \\[ \\text{argmax}\\{f(x)\\} = \\frac{d}{dx}f(x) \\overset{set}{=} 0 \\] \\[ f(x) = 2400x - 2x^2 \\] \\[ \\begin{aligned} \\text{argmax}\\{f(x)\\} = \\frac{d}{dx} 2400x - 2x^2 = 2400 - 4x \\overset{set}{=} 0 \\\\ \\\\ 2400 = 4x \\\\ \\\\ 600 = x \\end{aligned} \\] Check second derivative \\[ \\frac{d}{d^2 x} 2400x - 2x^2 = \\frac{d}{dx} 2400 - 4x = -4 &lt; 0 \\] \\(x = 600\\) is the absolute maximum of \\(f(x)\\) \\(f(x)\\) is concave down Programming languages like Python and Wolfram can use argmax() and argmin() functions In R we use optim() We’ll discuss later This is our “why do we care” for derivatives Integrals “Anti-derivative” \\[ \\frac{d}{dx}x^2 = 2x \\qquad\\qquad \\int 2x \\ dx = \\frac{2x^2}{2} + C = x^2 + C \\] C is a constant “indefinite” integrals assume nothing about the derivative \\[ \\frac{d}{dx}x^2 + 1000 = 2x = \\frac{d}{dx}x^2 \\] Integrals are the area underneath a curve We can set definite boundaries to look at interval areas \\[ \\int_{1}^2 \\frac{1}{x} \\ dx = \\ln(|2|) - \\ln(|1|) \\approx 0.693 \\] Why do we care? \\[ X \\sim \\text{Exp}(\\theta) \\] \\[ f_x(x) = \\theta e^{-\\theta x} \\] \\[ \\mathbb{E}X = \\int_0^{\\infty} x f_x(x) \\ dx = \\int_0^{\\infty} x\\theta e^{-\\theta x} \\ dx = \\frac{1}{\\theta} \\] \\[ \\mathbb{E}X^2 = \\int_0^{\\infty} x^2 f_x(x) \\ dx = \\int_0^{\\infty} x^2 \\theta e^{-\\theta x} \\ dx = \\frac{2}{\\theta^2} \\] \\[ \\mathbb{V}X = \\mathbb{E}X^2 - [\\mathbb{E}X]^2 = \\frac{2}{\\theta^2} - \\left( \\frac{1}{\\theta} \\right)^2 = \\frac{1}{\\theta^2} \\] Numerical integration Riemann sums Taught before integrals Area of rectangles underneath integral Accuracy is based on number of rectangles Not great to implement Only good for univariate \\[ f(x) = x^2 - 2x + 3 \\] \\[ \\begin{aligned} \\int_1^3 x^2 - 2x + 3 \\ dx = \\left. \\frac{x^3}{3}- x^2 + 3x \\ \\right|_1^3 =\\\\ \\\\ \\left(\\frac{3^3}{3} - 3^2 + 3(3) \\right) - \\left(\\frac{1^3}{3}- 1^2 + 3(1) \\right) =\\\\ \\\\ (3 - 9 + 9) - \\left(\\frac{1}{3} - 4 \\right) =\\\\ \\\\ 6 \\frac{2}{3} \\approx 6.67 \\\\ \\end{aligned} \\] Monte Carlo integration Simulate values within the interval Evaluate the function at those values Take their mean Weight it by the interval Very reliable, can handle multidimensional fx = function(x){x^2 - 2*x + 3} a = 1 # lower bound b = 3 # upper bound n = 100000 # n samples # random simulation between a and b x = runif(n, min = a, max = b) # evaluate f(x) at simulations mc_sim = fx(x) # mean weighted by difference mc_int = (b - a)*mean(mc_sim) mc_int ## [1] 6.66993 Why do we care? \\[ X \\sim \\text{Beta}(4,10) \\] \\[ \\mathbb{E}X = ? \\qquad\\qquad \\mathbb{V}X = ? \\] # simulate 10000 beta(4,10) r.v. realizations mc_sim = rbeta(10000,4,10) mean(mc_sim) # empirical mean ## [1] 0.2861241 var(mc_sim) # variance ## [1] 0.0138754 \\[ \\mathbb{E}X = \\frac{4}{4+10} = \\frac{4}{14} \\approx 0.285 \\] \\[ \\mathbb{V}X = \\frac{4 \\times 10}{(4 + 10)^2 (4 + 10 + 1)} = \\frac{40}{196 \\times 15} = \\frac{40}{2940} \\approx 0.0136 \\] 2.4 Advanced Math Traditionally, statistics involves proofs Supplemental material includes them Not required (or really recommended) Stats majors (?) Math focused “class choice” topics Differential equations Worth discussing for the ecologists (a little for Epi students) No calculus needed, algebraic “difference” equations Graph theory / Networks Very useful, very topical (AI/LLMs/ML) More important for Agronomy/Plant path/Epi students Bayesian “Useful” to everyone Difficult to understand Need strong comprehension of integrals/expected values 2.5 Next class Bring laptops Have R downloaded + RStudio Equivalent if you’re using Python Talk to me if any of the above is an issue We will have a “Lab” 2.6 Assignment 1 2.7 Go away "],["day-3.html", "3 Day 3 3.1 Review 3.2 R Programming 3.3 R Programming Lab 3.4 Go away", " 3 Day 3 3.1 Review \\[ \\text{Matrix: } \\textbf{A} = \\begin{bmatrix}1 &amp; 2 \\\\ 3 &amp; 4\\end{bmatrix} \\qquad\\qquad \\text{Vector: } \\boldsymbol{x} = \\begin{bmatrix}1 \\\\ 2 \\\\ 3\\end{bmatrix} \\qquad\\qquad \\text{Scalar: } a = 5 \\] Sums \\[ \\textbf{A} = \\begin{bmatrix}1 &amp; 2 \\\\ 3 &amp; 4\\end{bmatrix} \\qquad\\qquad \\textbf{B} = \\begin{bmatrix}1 &amp; 1 \\\\ 1 &amp; 0\\end{bmatrix} \\] \\[ \\textbf{A} + \\textbf{B} = \\begin{bmatrix}1 &amp; 2 \\\\ 3 &amp; 4\\end{bmatrix} + \\begin{bmatrix}1 &amp; 0 \\\\ 1 &amp; 0\\end{bmatrix} = \\begin{bmatrix}2 &amp; 2 \\\\ 4 &amp; 4\\end{bmatrix} \\] Vectors/Matrices multiplied by Scalars \\[ 3 \\times \\begin{bmatrix}1 &amp; 2 \\\\ 3 &amp; 4\\end{bmatrix} = \\begin{bmatrix}3 &amp; 6 \\\\ 9 &amp; 12\\end{bmatrix} \\qquad\\qquad 10 \\times \\begin{bmatrix}1 \\\\ 2 \\\\ 3\\end{bmatrix} = \\begin{bmatrix}10 \\\\ 20 \\\\ 30\\end{bmatrix} \\] Transpose \\[ \\textbf{A} = \\begin{bmatrix}1 &amp; 2 \\\\ 3 &amp; 4 \\\\ 5 &amp; 6 \\end{bmatrix} \\qquad\\qquad \\textbf{A}^\\prime = \\begin{bmatrix}1 &amp; 3 &amp; 5\\\\ 2 &amp; 4 &amp; 6 \\end{bmatrix} \\] Matrix products \\[ \\begin{bmatrix} 1 &amp; 3 &amp; 5 \\\\ 2 &amp; 4 &amp; 6 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\\\ 5 &amp; 6 \\end{bmatrix} = \\begin{bmatrix} (1 \\cdot 1) + (3 \\cdot 3) + (5 \\cdot 5) &amp; (1 \\cdot 2) + (3 \\cdot 4) + (5 \\cdot 6) \\\\ (2 \\cdot 1) + (4 \\cdot 3) + (6 \\cdot 5) &amp; (2 \\cdot 2) + (4 \\cdot 4) + (6 \\cdot 6) \\end{bmatrix} = \\begin{bmatrix} 35 &amp; 44 \\\\ 44 &amp; 56 \\end{bmatrix} \\] Determinants \\[ \\textbf{A} = \\begin{bmatrix} 0 &amp; -2 \\\\ 2 &amp; 0 \\end{bmatrix} \\] \\[ \\text{det}(\\textbf{A}) = \\text{det} \\left(\\begin{bmatrix} 0 &amp; -2 \\\\ 2 &amp; 0 \\end{bmatrix} \\right) = ad - bc = (0 \\times 0) - (-2 \\times 2) = 4 \\] Matrix inverse \\[ \\textbf{A} = \\begin{bmatrix}0 &amp; -1 \\\\ 1 &amp; 0\\end{bmatrix} \\qquad\\qquad \\textbf{A}^{-1} = \\begin{bmatrix}0 &amp; 1 \\\\ -1 &amp; 0\\end{bmatrix} \\] \\[ \\textbf{AA}^{-1} = \\begin{bmatrix}0 &amp; -1 \\\\ 1 &amp; 0\\end{bmatrix}\\begin{bmatrix}0 &amp; 1 \\\\ -1 &amp; 0\\end{bmatrix} = \\begin{bmatrix}1 &amp; 0 \\\\ 0 &amp; 1\\end{bmatrix} = \\textbf{I} \\] Method of least squares \\[ y_i = \\beta_0 + \\beta_1x_i + \\beta_2z_i + \\epsilon_i \\] \\[ \\boldsymbol{y} = \\boldsymbol{X \\beta} + \\boldsymbol{\\epsilon} \\] \\[ \\hat{\\boldsymbol{\\beta}} = (\\boldsymbol{X}^\\prime \\boldsymbol{X})^{-1} \\boldsymbol{X}^\\prime \\boldsymbol{y} \\] x1 = cruise$start_month # predictor 1 x2 = cruise$voyage_duration # predictor 2 y = cruise$infected # response X = cbind(1,x1,x2) # design matrix # matrix method of least squares solve(t(X)%*%X)%*%t(X)%*%y ## [,1] ## 179.700447 ## x1 -5.217664 ## x2 -1.889147 # base R multiple regression coef(lm(y ~ x1 + x2)) ## (Intercept) x1 x2 ## 179.700447 -5.217664 -1.889147 Derivatives \\[ \\frac{d}{dx} x^2 = 2x \\qquad\\qquad \\frac{d}{dx} 8x = 8 \\qquad\\qquad \\frac{d}{dx} 9 = 0 \\] \\[ \\frac{d}{dx} x^2 - 8x + 9 = 2x - 8 \\] #limit definition of derivative lim_diff = function(f, x, h = 1e-5) { # h should always be a small step size ddx = (f(x + h) - f(x)) / h return(ddx) } fx = function(x){x^2 - 8*x + 9} lim_diff(fx,2) # evaluate at x = 2 ## [1] -3.99999 Optimization \\[ \\text{argmax}\\{f(x)\\} = \\frac{d}{dx}f(x) \\overset{set}{=} 0 \\] \\[ f(x) = 2400x - 2x^2 \\] \\[ \\begin{aligned} \\text{argmax}\\{f(x)\\} = \\frac{d}{dx} 2400x - 2x^2 = 2400 - 4x \\overset{set}{=} 0 \\\\ \\\\ 2400 = 4x \\\\ \\\\ 600 = x \\end{aligned} \\] Check second derivative \\[ \\frac{d}{d^2 x} 2400x - 2x^2 = \\frac{d}{dx} 2400 - 4x = -4 &lt; 0 \\] \\(x = 600\\) is the absolute maximum of \\(f(x)\\) \\(f(x)\\) is concave down Integrals \\[ \\frac{d}{dx}x^2 = 2x \\qquad\\qquad \\int 2x \\ dx = \\frac{2x^2}{2} + C = x^2 + C \\] \\[ \\int_{1}^2 \\frac{1}{x} \\ dx = \\ln(|2|) - \\ln(|1|) \\approx 0.693 \\] \\[ X \\sim \\text{Exp}(\\theta) \\] \\[ f_x(x) = \\theta e^{-\\theta x} \\] \\[ \\mathbb{E}X = \\int_0^{\\infty} x f_x(x) \\ dx = \\int_0^{\\infty} x\\theta e^{-\\theta x} \\ dx = \\frac{1}{\\theta} \\] \\[ \\mathbb{E}X^2 = \\int_0^{\\infty} x^2 f_x(x) \\ dx = \\int_0^{\\infty} x^2 \\theta e^{-\\theta x} \\ dx = \\frac{2}{\\theta^2} \\] \\[ \\mathbb{V}X = \\mathbb{E}X^2 - [\\mathbb{E}X]^2 = \\frac{2}{\\theta^2} - \\left( \\frac{1}{\\theta} \\right)^2 = \\frac{1}{\\theta^2} \\] Numerical integration \\[ f(x) = x^2 - 2x + 3 \\] \\[ \\begin{aligned} \\int_1^3 x^2 - 2x + 3 \\ dx = \\left. \\frac{x^3}{3}- x^2 + 3x \\ \\right|_1^3 =\\\\ \\\\ \\left(\\frac{3^3}{3} - 3^2 + 3(3) \\right) - \\left(\\frac{1^3}{3}- 1^2 + 3(1) \\right) =\\\\ \\\\ (3 - 9 + 9) - \\left(\\frac{1}{3} - 4 \\right) =\\\\ \\\\ 6 \\frac{2}{3} \\approx 6.67 \\\\ \\end{aligned} \\] Monte Carlo integration fx = function(x){x^2 - 2*x + 3} a = 1 # lower bound b = 3 # upper bound n = 100000 # n samples # random simulation between a and b x = runif(n, min = a, max = b) # evaluate f(x) at simulations mc_sim = fx(x) # mean weighted by difference mc_int = (b - a)*mean(mc_sim) mc_int ## [1] 6.663756 \\[ X \\sim \\text{Beta}(4,10) \\] \\[ \\mathbb{E}X = ? \\qquad\\qquad \\mathbb{V}X = ? \\] # simulate 10000 beta(4,10) r.v. realizations mc_sim = rbeta(10000,4,10) mean(mc_sim) # empirical mean ## [1] 0.2866834 var(mc_sim) # variance ## [1] 0.01367972 \\[ \\mathbb{E}X = \\frac{4}{4+10} = \\frac{4}{14} \\approx 0.285 \\] \\[ \\mathbb{V}X = \\frac{4 \\times 10}{(4 + 10)^2 (4 + 10 + 1)} = \\frac{40}{196 \\times 15} = \\frac{40}{2940} \\approx 0.0136 \\] Any questions? 3.2 R Programming If you’re using Python, be warned Python use means you’re too comfortable to switch I expect clean, decently optimized code Strong rationale for every library Excel Possible, reasonable Not recommended Program R “Scripting” language Open-source (a.k.a. Free) Highly accessible Like Python, but easier to learn + slightly more performant General structure Not a programming class Today: quick overview, basic principles Tomorrow: R Markdown and simple LaTeX That’s it, the rest is on you Follow along + start messing around on your own 3.2.1 R Studio “Interactive Development Environment” or IDE for short Makes using and learning R much easier Hands down the best “R only” IDE Other IDEs Virtual Studio JetBrains VIM My take: don’t obsess. RStudio for R, VSCode if you become a programmer. VIM users are the coffee snobs of tech R Studio walk-through 3.2.2 Basic principles Documentation/commenting code Every non-redundant line # mtcars base R dataset data = mtcars y = data$mpg # mpg x1 = data$hp # horsepower x2 = data$wt # weight # linear model predicting mpg with horsepower m1 = lm(y ~ x1) # predict mpg with weight m2 = lm(y ~ x2) # predict mpg with both m3 = lm(y ~ x1 + x2) # coefficients for each model coef(m1) ## (Intercept) x1 ## 30.09886054 -0.06822828 coef(m2) ## (Intercept) x2 ## 37.285126 -5.344472 coef(m3) ## (Intercept) x1 x2 ## 37.22727012 -0.03177295 -3.87783074 Helper functions Build early, build often Separate files (obviously can’t do here) library(dplyr) # tidyverse data wrangling # function to calculate means for x based on strata strata_means = function(data,strata,x){ out = data %&gt;% # group by strata group_by(.data[[strata]]) %&gt;% # calculate mean of x per strata summarise(xbar = mean(.data[[x]])) # rename columns colnames(out) = c(strata,x) return(out) # return dataframe of means } # run function strata_means(data,&quot;vs&quot;,&quot;mpg&quot;) ## # A tibble: 2 × 2 ## vs mpg ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 16.6 ## 2 1 24.6 You would save this as strata_means.R and call it with source(\"strata_means.R\") Then you could run the function Prevents “How did I do that thing?” Easier to troubleshoot errors Explanatory code Objects/elements/function are named in ways that don’t require comments Naming conventions camelCase snake_case PascalCase kebab-case UPPER_CASE S.case (common in R) Just be consistent Switching constantly creates confusion I use a bastardized version of snake_case You probably shouldn’t copy me But that’s between you and your creator # the right way to do snake_case in R # arrows to define objects cars_data &lt;- mtcars # equals for inline definition + underscores on all separators mpg_model_1 &lt;- lm(mpg ~ hp, data = cars_data) # consistent spacing for code &amp; comments model_1_coefs &lt;- coef(mpg_model_1) # clear and explanatory code print(model_1_coefs) ## (Intercept) hp ## 30.09886054 -0.06822828 # my jacked up way # uppercase when the vibe fits DOC_address = &quot;data/DOCYearEndReportAllData.xlsx&quot; # equal sign instead of arrows always for max laziness polys = st_read(&quot;data/WBDHU8/WBDHU8mod.shp&quot;) # inconsistent spacing of functions kansas_map = maps::map(&quot;state&quot;, &quot;kansas&quot;, fill = TRUE, plot = FALSE) kansas_sf = st_as_sf(kansas_map) |&gt; st_set_crs(4326) dat24 = daq[[4]] # numbers don&#39;t get underscore delims # no comment spacing / change above vs. inline comment whenever df_tkn = subset(dat24,dat24$Pollutant == &quot;TKN&quot;) # reversing naming conventions when the spirit guides me sf_df = st_as_sf(df_tkn, coords = c(&quot;Long&quot;,&quot;Lat&quot;), crs = 4326) You don’t read this mess without a function dictionary Spoiler alert: I never make those Error catching Rather advanced technique Still probably good to learn (?) I’m bad about it # function to calculate means for x based on strata strata_means = function(data,strata,x){ # error catch for non-numeric inputs if(is.numeric(data[[x]]) == FALSE){ cat(&quot;Error: Variable input to be averaged is non-numeric.&quot;, &quot;\\n&quot;, &quot;Please check str(data[[x]]) and confirm.&quot;, &quot;\\n&quot;, &quot;Change to as.numeric(data[[x]]) if this is intended.&quot;) } else{ out = data %&gt;% # group by strata group_by(.data[[strata]]) %&gt;% # calculate mean of x per strata summarise(xbar = mean(.data[[x]])) # rename columns colnames(out) = c(strata,x) return(out) # return dataframe of means } } # run function data$mpg = as.character(data$mpg) strata_means(data,&quot;vs&quot;,&quot;mpg&quot;) ## Error: Variable input to be averaged is non-numeric. ## Please check str(data[[x]]) and confirm. ## Change to as.numeric(data[[x]]) if this is intended. 3.2.3 Live example Code 3.3 R Programming Lab Partner up if you so choose. Complete the following with the seed “73”: Define 3 scalars \\[ a = 3 \\qquad\\qquad b = 5 \\qquad\\qquad c = 0.25 \\] Define 3 vectors \\[ \\boldsymbol{x} = \\begin{bmatrix} 5 \\\\ 3 \\\\ 6 \\end{bmatrix} \\qquad\\qquad \\boldsymbol{y} = \\begin{bmatrix} 11.3 \\\\ 8.97 \\\\ 9.82 \\end{bmatrix} \\qquad\\qquad \\boldsymbol{z} = \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix} \\] Generate 3 standard normal values and save them to a variable named w. Multiply w by a random uniform value bound between 1 and 5, then save it as the new w. Define the matrix: \\[ \\textbf{A} = \\begin{bmatrix} \\boldsymbol{x} + w_1 \\\\ \\boldsymbol{y} + w_2 \\\\ \\boldsymbol{z} + w_3 \\end{bmatrix} \\] Confirm the following is true: \\[ \\textbf{AA}^{-1} = \\textbf{I} \\] Add a row of 3 gamma distributed values with shape equal to 2 and rate equal to 1/5. Simulate 4 normally distributed data points with a mean equal to the trace of A and standard deviation of 5. Save them as a variable named q. Add a column of 1s to the matrix A and define: \\[ \\hat{\\boldsymbol{\\theta}} = (\\boldsymbol{A}^\\prime \\boldsymbol{A})^{-1} \\boldsymbol{A}^\\prime \\boldsymbol{q} \\] Calculate the predicted values of q: \\[ \\hat{\\boldsymbol{q}} = \\boldsymbol{A} \\boldsymbol{\\hat{\\theta}} \\] 3.4 Go away "],["day-4.html", "4 Day 4 4.1 Review 4.2 (More) Advanced stuff 4.3 Advanced Programming 4.4 In-class activity 4.5 Go away", " 4 Day 4 4.1 Review PSA: I’m teaching “bad programming” Don’t send me hate mail years later if you properly learn I know I’m dis-servicing you all but we have limited time For example: We won’t discuss OOP, S3 vectors, vectorization Leave it in the TEVAL if you think I should teach a course on it 4.1.1 Basic principles Documentation/commenting code Every non-redundant line # mtcars base R dataset data = mtcars y = data$mpg # mpg x1 = data$hp # horsepower x2 = data$wt # weight # linear model predicting mpg with horsepower m1 = lm(y ~ x1) # predict mpg with weight m2 = lm(y ~ x2) # predict mpg with both m3 = lm(y ~ x1 + x2) # coefficients for each model coef(m1) ## (Intercept) x1 ## 30.09886054 -0.06822828 coef(m2) ## (Intercept) x2 ## 37.285126 -5.344472 coef(m3) ## (Intercept) x1 x2 ## 37.22727012 -0.03177295 -3.87783074 Helper functions Build early, build often library(dplyr) # tidyverse data wrangling # function to calculate means for x based on strata strata_means = function(data,strata,x){ out = data %&gt;% # group by strata group_by(.data[[strata]]) %&gt;% # calculate mean of x per strata summarise(xbar = mean(.data[[x]])) # rename columns colnames(out) = c(strata,x) return(out) # return dataframe of means } # run function strata_means(data,&quot;vs&quot;,&quot;mpg&quot;) ## # A tibble: 2 × 2 ## vs mpg ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 16.6 ## 2 1 24.6 You would save this as strata_means.R and call it with source(\"strata_means.R\") Then you could run the function Prevents “How did I do that thing?” Easier to troubleshoot errors Explanatory code Objects/elements/function are named in ways that don’t require comments Naming conventions camelCase snake_case PascalCase kebab-case UPPER_CASE S.case (common in R) Just be consistent # the right way to do snake_case in R # arrows to define objects cars_data &lt;- mtcars # equals for inline definition + underscores on all separators mpg_model_1 &lt;- lm(mpg ~ hp, data = cars_data) # consistent spacing for code &amp; comments model_1_coefs &lt;- coef(mpg_model_1) # clear and explanatory code print(model_1_coefs) ## (Intercept) hp ## 30.09886054 -0.06822828 Error catching # function to calculate means for x based on strata strata_means = function(data,strata,x){ # error catch for non-numeric inputs if(is.numeric(data[[x]]) == FALSE){ cat(&quot;Error: Variable input to be averaged is non-numeric.&quot;, &quot;\\n&quot;, &quot;Please check str(data[[x]]) and confirm.&quot;, &quot;\\n&quot;, &quot;Change to as.numeric(data[[x]]) if this is intended.&quot;) } else{ out = data %&gt;% # group by strata group_by(.data[[strata]]) %&gt;% # calculate mean of x per strata summarise(xbar = mean(.data[[x]])) # rename columns colnames(out) = c(strata,x) return(out) # return dataframe of means } } # run function data$mpg = as.character(data$mpg) strata_means(data,&quot;vs&quot;,&quot;mpg&quot;) ## Error: Variable input to be averaged is non-numeric. ## Please check str(data[[x]]) and confirm. ## Change to as.numeric(data[[x]]) if this is intended. 4.1.2 R Basics Data types # scalars/variables a = 5 a ## [1] 5 # vectors x = c(1,2,3) x ## [1] 1 2 3 # matrices A = matrix(c(x,x-1,x+3),3,3,T) A ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 0 1 2 ## [3,] 4 5 6 # data frame df = data.frame(fruit = c(&quot;Apple&quot;, &quot;Banana&quot;, &quot;Orange&quot;), ID = c(111,112,113), Count = c(5,12,2)) df ## fruit ID Count ## 1 Apple 111 5 ## 2 Banana 112 12 ## 3 Orange 113 2 # list ls = list(x = x, A = A, df = df) ls ## $x ## [1] 1 2 3 ## ## $A ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 0 1 2 ## [3,] 4 5 6 ## ## $df ## fruit ID Count ## 1 Apple 111 5 ## 2 Banana 112 12 ## 3 Orange 113 2 Data elements # access vector elements by position x[1] ## [1] 1 x[3] ## [1] 3 # matrix elements: [row,column] A ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 0 1 2 ## [3,] 4 5 6 A[3,1] ## [1] 4 # dataframe$column or dataframe[[&quot;column&quot;]] df$fruit ## [1] &quot;Apple&quot; &quot;Banana&quot; &quot;Orange&quot; df[[&quot;fruit&quot;]] ## [1] &quot;Apple&quot; &quot;Banana&quot; &quot;Orange&quot; # list[[element]] ls[[2]] ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 0 1 2 ## [3,] 4 5 6 ls[[2]][3,1] ## [1] 4 Data actions # change the elements of a dataframe column df$ID = c(1,2,3) df$ID ## [1] 1 2 3 rownames(df) = c(&quot;R1&quot;,&quot;R2&quot;,&quot;R3&quot;) # rename rows colnames(df) = c(&quot;C1&quot;,&quot;C2&quot;,&quot;C3&quot;) # rename columns df ## C1 C2 C3 ## R1 Apple 1 5 ## R2 Banana 2 12 ## R3 Orange 3 2 Booleans df$C1 ## [1] &quot;Apple&quot; &quot;Banana&quot; &quot;Orange&quot; df$C1 == &quot;Apple&quot; # print true where apple is present ## [1] TRUE FALSE FALSE df$C1 != &quot;Apple&quot; # print true when not apple ## [1] FALSE TRUE TRUE Extended print statements # single line cat(&quot;The mean of column 3 is:&quot;, mean(df$C3), &quot;\\n&quot;) ## The mean of column 3 is: 6.333333 # multiple lines using &quot;\\n&quot; for breaks cat(&quot;The variance of column 3 is:&quot;, var(df$C3), &quot;\\n&quot;, &quot;The standard deviation of column 3 is:&quot;, sd(df$C3)) ## The variance of column 3 is: 26.33333 ## The standard deviation of column 3 is: 5.131601 Removing NA values df[1,1] = NA na.omit(df) # removes all rows with NAs ## C1 C2 C3 ## R2 Banana 2 12 ## R3 Orange 3 2 df[,colSums(is.na(df)) == 0] # remove all columns with NAs ## C2 C3 ## R1 1 5 ## R2 2 12 ## R3 3 2 Replacing NA values df[is.na(df)] = 0 df ## C1 C2 C3 ## R1 0 1 5 ## R2 Banana 2 12 ## R3 Orange 3 2 Any questions? 4.2 (More) Advanced stuff For loops # mtcars data data = mtcars # split by vs splt = split(mtcars,mtcars$vs) # empty vector out = c() # &quot;for each element of the list&quot; for(i in 1:length(splt)){ # fill the vector with the mean mpg # of each list element out[i] = mean(splt[[i]]$mpg) } out # matches the strata_means() output ## [1] 16.61667 24.55714 Concatenate # a way to push extended strings onto objects names(out) = paste0(&quot;vs = &quot;, names(splt), sep = &quot; &quot;) out # labels better match strata_means() ## vs = 0 vs = 1 ## 16.61667 24.55714 Libraries/packages # install.packages(&quot;dplyr&quot;) library(dplyr) # we&#39;ve seen this a lot # install.packages(&quot;readxl&quot;) library(readxl) # excel files can&#39;t be read by default in R # install.packages(&quot;nlme&quot;) library(nlme) # generalized least squares # important for next week WARNING: If you use a package I haven’t shown you in class Bad things occur Exception: ggplot2 (don’t use geom_smooth) Queue soap box That’s the majority of it We’ll learn more over time 4.3 Advanced Programming We don’t need strong skills to succeed here AI/LLMs Don’t let them write your code entirely “Packing a suit case” Use them like search engines Good for “canned” solutions I always know when you use AI I rarely say anything unless its bad Don’t be bad if you decide to use them Learning sometimes involves pain, programming requires it The fastest way out is through If you like this stuff, feel free to dive deep I’m happy to help Programming focused “class choice” topics High dimensional data analysis (more parameters than samples) Very important for genetics/animal science Convex optimization problems, nasty stuff Abuse packages to survive Neural networks Agronomy/animal science Fusion between “mathy” and “programming-y” We’ll talk about machine learning anyway, this is “advanced” ML Monte Carlo / Markov Chain Monte Carlo The programming solution to Bayesian Sampling algorithm Useful for anyone considering computational biology/genetics 4.4 In-class activity R Markdown Assignment 2 4.5 Go away "],["day-5.html", "5 Day 5 5.1 Review 5.2 Statistical Modeling 5.3 Stringing sentences 5.4 Telling stories 5.5 Go away", " 5 Day 5 5.1 Review Documentation/commenting code # mtcars base R dataset data = mtcars y = data$mpg # mpg x1 = data$hp # horsepower x2 = data$wt # weight # linear model predicting mpg with horsepower m1 = lm(y ~ x1) # predict mpg with weight m2 = lm(y ~ x2) # predict mpg with both m3 = lm(y ~ x1 + x2) # coefficients for each model coef(m1) coef(m2) coef(m3) Helper functions library(dplyr) # tidyverse data wrangling # function to calculate means for x based on strata strata_means = function(data,strata,x){ out = data %&gt;% # group by strata group_by(.data[[strata]]) %&gt;% # calculate mean of x per strata summarise(xbar = mean(.data[[x]])) # rename columns colnames(out) = c(strata,x) return(out) # return dataframe of means } # run function strata_means(data,&quot;vs&quot;,&quot;mpg&quot;) Explanatory code Objects/elements/function are named in ways that don’t require comments Naming conventions camelCase snake_case PascalCase kebab-case UPPER_CASE S.case (common in R) Error catching # function to calculate means for x based on strata strata_means = function(data,strata,x){ # error catch for non-numeric inputs if(is.numeric(data[[x]]) == FALSE){ cat(&quot;Error: Variable input to be averaged is non-numeric.&quot;, &quot;\\n&quot;, &quot;Please check str(data[[x]]) and confirm.&quot;, &quot;\\n&quot;, &quot;Change to as.numeric(data[[x]]) if this is intended.&quot;) } else{ out = data %&gt;% # group by strata group_by(.data[[strata]]) %&gt;% # calculate mean of x per strata summarise(xbar = mean(.data[[x]])) # rename columns colnames(out) = c(strata,x) return(out) # return dataframe of means } } # run function data$mpg = as.character(data$mpg) strata_means(data,&quot;vs&quot;,&quot;mpg&quot;) ## Error: Variable input to be averaged is non-numeric. ## Please check str(data[[x]]) and confirm. ## Change to as.numeric(data[[x]]) if this is intended. Data types # scalars/variables a = 5 a # vectors x = c(1,2,3) x # matrices A = matrix(c(x,x-1,x+3),3,3,T) A # data frame df = data.frame(fruit = c(&quot;Apple&quot;, &quot;Banana&quot;, &quot;Orange&quot;), ID = c(111,112,113), Count = c(5,12,2)) df # list ls = list(x = x, A = A, df = df) ls Data elements # access vector elements by position x[1] x[3] # matrix elements: [row,column] A A[3,1] # dataframe$column or dataframe[[&quot;column&quot;]] df$fruit df[[&quot;fruit&quot;]] # list[[element]] ls[[2]] ls[[2]][3,1] Data actions # change the elements of a dataframe column df$ID = c(1,2,3) df$ID rownames(df) = c(&quot;R1&quot;,&quot;R2&quot;,&quot;R3&quot;) # rename rows colnames(df) = c(&quot;C1&quot;,&quot;C2&quot;,&quot;C3&quot;) # rename columns df Booleans df$C1 df$C1 == &quot;Apple&quot; # print true where apple is present df$C1 != &quot;Apple&quot; # print true when not apple Extended print statements # single line cat(&quot;The mean of column 3 is:&quot;, mean(df$C3), &quot;\\n&quot;) # multiple lines using &quot;\\n&quot; for breaks cat(&quot;The variance of column 3 is:&quot;, var(df$C3), &quot;\\n&quot;, &quot;The standard deviation of column 3 is:&quot;, sd(df$C3)) Removing NA values df[1,1] = NA na.omit(df) # removes all rows with NAs df[,colSums(is.na(df)) == 0] # remove all columns with NAs Replacing NA values df[is.na(df)] = 0 df For loops (Note on C programming) # mtcars data data = mtcars # split by vs splt = split(mtcars,mtcars$vs) # empty vector out = c() # &quot;for each element of the list&quot; for(i in 1:length(splt)){ # fill the vector with the mean mpg # of each list element out[i] = mean(splt[[i]]$mpg) } out # matches the strata_means() output Concatenate # a way to push extended strings onto objects names(out) = paste0(&quot;vs = &quot;, names(splt), sep = &quot; &quot;) out # labels better match strata_means() Libraries/packages # install.packages(&quot;dplyr&quot;) library(dplyr) # we&#39;ve seen this a lot # install.packages(&quot;readxl&quot;) library(readxl) # excel files can&#39;t be read by default in R # install.packages(&quot;nlme&quot;) library(nlme) # generalized least squares # important for next week R Markdown “Graves” ” ` ” {r}, {r, echo=FALSE, message=FALSE, warning=FALSE} {fig.align=‘center’} HTML vs PDF Headers # First level ## Second level ### Third level - Bullet * Bullet + Bullet 1. Numbered bullet a. Letter bullet &gt; Quote block Quarto vs. Rmd LaTeX $ vs $$ vs \\[ \\beta, \\gamma \\bar{}, \\hat{} \\begin{array}, \\begin{bmatrix}, \\begin{aligned} \\vspace{3mm}, \\newpage Mean and standard deviation \\[ \\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i \\qquad\\qquad s = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}(x_i - \\bar{x})^2} \\] # base R dataset on fiji earthquakes quakes = datasets::quakes # mean magnitude mean(quakes$mag) ## [1] 4.6204 # variance of magnitude var(quakes$mag) ## [1] 0.1622261 # standard deviation of magnitude sd(quakes$mag) ## [1] 0.402773 Quantiles/Quartiles \\[ \\begin{array}{|c|c|c|c|c|} \\hline \\text{Min} &amp; \\text{Q}_1 &amp; \\text{Median} &amp; \\text{Q}_3 &amp; \\text{Max} \\\\ \\hline 0^{th} &amp; 25^{th} &amp; 50^{th} &amp; 75^{th} &amp; 100^{th} \\\\ \\hline \\end{array} \\] # five number summary as quantiles quantile(quakes$mag, c(0,0.25,0.5,0.75,1)) ## 0% 25% 50% 75% 100% ## 4.0 4.3 4.6 4.9 6.4 # five number summary base R function fivenum(quakes$mag) ## [1] 4.0 4.3 4.6 4.9 6.4 Histograms and Boxplots # base R histogram hist(quakes$mag, xlab = &quot;Magnitude&quot;, # x axis label main = &quot;Earthquakes off Fiji&quot;, # title col = &quot;white&quot;) # bar color hist(quakes$mag, # data xlab = &quot;Magnitude&quot;, main = &quot;Earthquakes off Fiji&quot;, col = &quot;white&quot;) abline(v = mean(quakes$mag), # vertical mean line lty = 2, # line type (dashed) lwd = 2, # line width col = &quot;gold&quot;) # line color # margin control par(mar = c(3,1,1,1)) # bottom, left, top, right # base R boxplot boxplot(quakes$mag, horizontal = T, # set as horizontal (T = TRUE) col = &quot;white&quot;) # central box color par(mar = c(3,1,1,1)) boxplot(quakes$mag, horizontal = T, col = &quot;white&quot;) abline(v = c(4,4.3,4.6,4.9,5.7), # vertical quantile lines lty = 2, lwd = 2, # color palette col = c(&quot;#FED976&quot;,&quot;#FEB24C&quot;,&quot;#FD8D3C&quot;,&quot;#E31A1C&quot;,&quot;#800026&quot;)) text(c(4.1,4.24,4.5,4.82,5.5), # x position c(0.9,1.15,0.9,1.15,0.9), # y position c(&quot;Min&quot;,&quot;Q1&quot;,&quot;Med&quot;,&quot;Q3&quot;,&quot;Max&quot;), # quantile labels col = c(&quot;#FED976&quot;,&quot;#FEB24C&quot;,&quot;#FD8D3C&quot;,&quot;#E31A1C&quot;,&quot;#800026&quot;)) \\(z\\)-scores \\[ z = \\frac{x - \\bar{x}}{s} \\] # transform the entire dataset into z-scores z = (quakes$mag - mean(quakes$mag))/sd(quakes$mag) Normal distributions \\[ X \\sim N(\\mu,\\sigma^2) \\qquad\\qquad \\frac{X - \\mu}{\\sigma} = Z \\sim N(0,1) \\] par(mar = c(4.5,4.5,1,1)) curve(dnorm(x), # standard normal density xlim = c(-4,4), # x axis limits lwd = 2, xlab = &quot;X&quot;, ylab = &quot;P(X=x)&quot;) # y axis label # z score for a 5.5 magnitude earthquake z_score = (5.5 - mean(quakes$mag))/sd(quakes$mag) 1 - pnorm(z_score) # 1 - z probability (same as z-table) ## [1] 0.01448625 Scatterplots # base R scatterplot plot(quakes$stations,quakes$mag, # x then y xlab = &quot;Number of stations reporting&quot;, # x axis label ylab = &quot;Magnitude&quot;, # y axis label main = &quot;Earthquakes off Fiji&quot;, # title pch = 20, # point shape col = &quot;#00000080&quot;) # point color Least squares regression \\[ y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\] # predictor x = quakes$stations # response y = quakes$mag # base R linear model m = lm(y ~ x) coef(m) # coefficients ## (Intercept) x ## 4.09726756 0.01565421 plot(quakes$stations,quakes$mag, xlab = &quot;Number of stations reporting&quot;, ylab = &quot;Magnitude&quot;, main = &quot;Earthquakes off Fiji&quot;, pch = 20, col = &quot;#00000080&quot;) # add regression line lines(sort(quakes$stations),sort(m$fitted.values), lwd = 2, col = &quot;gold&quot;) # line color Confidence intervals # 95% confidence interval # mean magnitude xbar_mag = mean(quakes$mag) # standard deviation of magnitude s_mag = sd(quakes$mag) # sample size n_mag = as.numeric(nrow(quakes)) # margin of error moe_mag = 1.96*(s_mag/sqrt(n_mag)) xbar_mag - moe_mag # lower bound ## [1] 4.595436 xbar_mag # point estimate ## [1] 4.6204 xbar_mag + moe_mag # upper bound ## [1] 4.645364 Hypothesis tests # schizophrenic reaction time data schiz = data.frame(time = scan(&quot;data/schiz.txt&quot;,skip=5), schizophrenia = c(rep(&quot;no&quot;,30*11),rep(&quot;yes&quot;,30*6))) library(dplyr) # R data wrangling package sstats = schiz %&gt;% # group by yes/no schiz group_by(schizophrenia) %&gt;% # mean/var/sample size for yes/no strata summarise(xbar = mean(time), s2 = var(time), n = n()) # print dataframe sstats ## # A tibble: 2 × 4 ## schizophrenia xbar s2 n ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 no 310. 4209. 330 ## 2 yes 507. 69089. 180 \\[ \\begin{aligned} \\text{H}_0: \\mu_1 - \\mu_2 = 0 \\\\ \\\\ \\text{H}_a: \\mu_1 - \\mu_2 &lt; 0 \\end{aligned} \\] \\[ t^* = \\frac{(\\bar{x}_1 - \\bar{x}_2) + (\\mu_1 - \\mu_2)}{\\sqrt{(s_1^2/n_1) + (s_2^2/n_2)}} \\] # test statistics for independent difference in means tstar = as.numeric((sstats[1,2] - sstats[2,2])/ sqrt((sstats[1,3]/sstats[1,4]) + (sstats[2,3]/sstats[2,4]))) tstar ## [1] -9.877136 # p-vale for t-pivot test statistic pt(tstar,min(sstats[,4])-1) ## [1] 6.114281e-19 Discrete expected value \\[ \\mathbb{E}X = \\sum_x x P(X=x) \\] Binomial distribution Discrete, binary processes Success/failure \\[ X \\sim \\text{Bin}(n,p) \\qquad \\qquad \\mathbb{E}X = np \\qquad \\qquad \\mathbb{V}X = np(1-p) \\] Poisson distribution Positive discrete counts \\[ X \\sim \\text{Pois}(\\lambda) \\qquad \\qquad \\mathbb{E}X = \\mathbb{V}X = \\lambda \\] Geometric distribution Number of trials to reach success \\[ X \\sim \\text{Geom}(p) \\qquad \\qquad \\mathbb{E}X = 1/p \\] Hypergeometric distribution Geometric distribution Sampling without replacement \\[ X \\sim \\text{Hyper}(N,K,n) \\qquad \\qquad \\mathbb{E}X = n \\frac{K}{N} \\] Continuous expected values \\[ \\mathbb{E}X = \\int_{\\chi}x f_x(x) \\] Gamma distribution Positive continuous values \\[ X \\sim \\text{Gamma}(\\alpha,\\theta) \\qquad \\qquad \\mathbb{E}X = \\alpha \\theta \\qquad \\qquad \\mathbb{V}X = \\alpha \\theta^2 \\] Beta distribution Proportions/probabilities \\[ X \\sim \\text{Beta}(\\alpha,\\beta) \\qquad \\qquad \\mathbb{E}X = \\frac{\\alpha}{\\alpha + \\beta} \\qquad \\qquad \\mathbb{V}X = \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)} \\] Uniform distribution Equi-probable events Defined boundaries (a and b) \\[ X \\sim \\text{Unif}(a,b) \\qquad \\qquad \\mathbb{E}X = \\frac{1}{2}(b-a) \\qquad \\qquad \\mathbb{V}X = \\frac{1}{12}(b-a)^2 \\] Any questions? 5.2 Statistical Modeling What is a model? Scientific vs. Thought vs. Mathematical Statistical? Why do we use models? Trend analysis What’s happening with total kjeldahl nitrogen in Kansas? plot(kslr_tkn$Year, kslr_tkn$UOM, col=&quot;#00000060&quot;, xlab = &quot;Year&quot;, ylab = &quot;TKN (mg/L)&quot;) Simple linear regression \\[ y_i = \\beta_0 + \\beta_1 t_i + \\epsilon_i \\] \\[ \\epsilon_i \\sim N(0, \\sigma^2 \\textbf{I}) \\] Another way to write it: \\[ y_i \\sim N(\\mu,\\sigma^2 \\textbf{I}) \\] \\[ \\mu = \\beta_0 + \\beta_1 t_i \\] mod_df = data.frame(UOM = kslr_tkn$UOM, Year = kslr_tkn$Year, huc08 = kslr_tkn$huc08) m1 = lm(UOM ~ Year, data = mod_df) summary(m1) ## ## Call: ## lm(formula = UOM ~ Year, data = mod_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.1020 -0.4490 -0.2046 0.1621 12.0753 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 44.898799 3.430303 13.09 &lt;2e-16 *** ## Year -0.021873 0.001706 -12.82 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8546 on 4761 degrees of freedom ## Multiple R-squared: 0.03338, Adjusted R-squared: 0.03318 ## F-statistic: 164.4 on 1 and 4761 DF, p-value: &lt; 2.2e-16 or = order(mod_df$Year) plot(mod_df$Year, mod_df$UOM, col=&quot;#D1D1D180&quot;, xlab = &quot;Year&quot;, ylab = &quot;TKN (mg/L)&quot;) lines(mod_df$Year[or], predict(m1, type=&quot;response&quot;)[or], lwd = 2, col = &quot;#512885&quot;) What about space? Add a “cluster effect” \\[ y_{ij} = \\beta_0 + \\beta_1 t_i + \\beta_2 s_j \\epsilon_{ij} \\] \\[ \\epsilon_{ij} \\sim N(0, \\sigma^2 \\textbf{I}) \\] \\(s_j\\) pertains to the “HUC” levels Think of it as a categorical predictor Goes to \\(0\\) based on which HUC is present m2 = lm(UOM ~ Year + as.factor(huc08), data = mod_df) summary(m2) ## ## Call: ## lm(formula = UOM ~ Year + as.factor(huc08), data = mod_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.4040 -0.4201 -0.1632 0.1809 11.8143 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 44.617022 3.305166 13.499 &lt; 2e-16 *** ## Year -0.021749 0.001642 -13.242 &lt; 2e-16 *** ## as.factor(huc08)10250017 0.368838 0.072993 5.053 4.51e-07 *** ## as.factor(huc08)10270101 -0.187518 0.075699 -2.477 0.013278 * ## as.factor(huc08)10270102 -0.201293 0.068389 -2.943 0.003263 ** ## as.factor(huc08)10270103 -0.214657 0.081542 -2.632 0.008504 ** ## as.factor(huc08)10270104 0.004271 0.068111 0.063 0.950001 ## as.factor(huc08)10270205 0.430969 0.074249 5.804 6.88e-09 *** ## as.factor(huc08)10270207 0.292476 0.077379 3.780 0.000159 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8216 on 4754 degrees of freedom ## Multiple R-squared: 0.108, Adjusted R-squared: 0.1065 ## F-statistic: 71.98 on 8 and 4754 DF, p-value: &lt; 2.2e-16 Where are we headed? 5.3 Stringing sentences Correlative inference Which cruise liner should you avoid? par(mar = c(5, 9, 4, 2)) boxplot(cruise$infected_proportion ~ cruise$liner, xlab = &quot;Proportion infected&quot;, ylab = &quot;&quot;, main = &quot;Norovirus outbreaks on cruise liners&quot;, horizontal = TRUE, las = 1, cex.axis = 0.7, col = &quot;#D1D1D190&quot;) \\[ \\left[y|\\alpha, \\beta \\right] \\equiv \\text{Beta}(\\alpha,\\beta) \\] \\[ \\alpha = \\mu \\phi \\qquad \\beta = (1 - \\mu) \\phi \\qquad g(\\mu) = \\textbf{X}^\\prime \\boldsymbol{\\beta} \\] m1 = gam(infected_proportion ~ as.factor(liner) - 1, data = cruise, family = betar(link = &quot;logit&quot;)) Seems like Fred Olsen is a bad idea… Frequency distribution of cruise liners Half of it, that is… Liner Frequency Aida Cruises 3 Azamara Club Cruises 1 Carnival Cruise Line 2 Celebrity Cruises 4 Crystal Cruises 1 Cunard Line 1 Disney Cruise Line 1 Fred Olsen Cruises 1 Holland America Line 12 Norwegian Cruise Line 3 5.4 Telling stories What just happened? Are these results a problem? Is the context behind them bad? What is data? What is a statistic? What is it for? What are the limitations of statistics? Of science? A major step in telling a story Having an idea for the plot What makes a good plot? How do scientific inquiries happen? By accident (Grokking) By madness (Game theory) Taking risks (Chemotherapy) Via spite (Scale-free networks) “Things breaking” (?) Is Fred Olsen a potent vector of norovirus? Are Carnival cruises really that safe of an option? bbreg = function(){ # process model for(i in 1:n){ y[i] ~ dbeta(alpha[i], beta[i]) # mean/precision parameterization alpha[i] &lt;- mu[i] * phi beta[i] &lt;- (1-mu[i]) * phi # logit link function logit(mu[i]) &lt;- a + b*x[i] } # priors phi ~ dgamma(1,.01) a ~ dnorm(0,.1) b ~ dnorm(0,.1) } # setting up jags data jags_dat = list(y = cruise$infected_proportion, x = as.integer(factor(cruise$liner)), n = nrow(cruise)) # initial proposal values inits = list(a = .01, b = .01, phi = 1) # fitting the model bbreg_mcmc = jags.fit(jags_dat,c(&quot;a&quot;,&quot;b&quot;,&quot;phi&quot;), bbreg,inits,n.adapt=5000, n.update=5000,n.iter=100000, thin=1,n.chains=1) # reproducibility seed set.seed(73) # number of posterior samples K = nrow(samp) # set to carnival and fred olsen x_pred = c(3,8) a_post = samp[,1] # intercept post b_post = samp[,2] # effect par post phi_post = samp[,3] # precision par post # vectorized posterior logit(mu) predictions eta_post = outer(a_post, x_pred, FUN = function(ai, x) ai) + outer(b_post, x_pred) # &quot;un-do&quot; the link function mu_post = invlogit(eta_post) # alpha posterior predictions alpha_post = mu_post * phi_post # beta posterior predictions beta_post = (1 - mu_post) * phi_post # posterior predictions of y y_post = matrix(rbeta(length(alpha_post), alpha_post, beta_post), nrow = K, ncol = length(x_pred)) Supplemental material Project proposal samples STAT 341 dictionary Varying R resources Extra reading R for Data Science Linear Models in R Assignment 2 5.5 Go away "],["day-6.html", "6 Day 6 6.1 Review 6.2 Stringing sentences 6.3 Telling stories 6.4 Linear models 6.5 Parameter estimation 6.6 Live example 6.7 The path forward 6.8 Go away", " 6 Day 6 6.1 Review What is a model? Scientific vs. Thought vs. Mathematical Statistical? Why do we use models? Trend analysis What’s happening with total kjeldahl nitrogen in Kansas? Simple linear regression \\[ y_i = \\beta_0 + \\beta_1 t_i + \\epsilon_i \\] \\[ \\epsilon_i \\sim N(0, \\sigma^2 \\textbf{I}) \\] \\[ y_i \\sim N(\\mu,\\sigma^2 \\textbf{I}) \\] \\[ \\mu = \\beta_0 + \\beta_1 t_i \\] mod_df = data.frame(UOM = kslr_tkn$UOM, Year = kslr_tkn$Year, huc08 = kslr_tkn$huc08) m1 = lm(UOM ~ Year, data = mod_df) summary(m1) ## ## Call: ## lm(formula = UOM ~ Year, data = mod_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.1020 -0.4490 -0.2046 0.1621 12.0753 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 44.898799 3.430303 13.09 &lt;2e-16 *** ## Year -0.021873 0.001706 -12.82 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8546 on 4761 degrees of freedom ## Multiple R-squared: 0.03338, Adjusted R-squared: 0.03318 ## F-statistic: 164.4 on 1 and 4761 DF, p-value: &lt; 2.2e-16 or = order(mod_df$Year) plot(mod_df$Year, mod_df$UOM, col=&quot;#D1D1D180&quot;, xlab = &quot;Year&quot;, ylab = &quot;TKN (mg/L)&quot;) lines(mod_df$Year[or], predict(m1, type=&quot;response&quot;)[or], lwd = 2, col = &quot;#512885&quot;) What about space? Add a “cluster effect” \\[ y_{ij} = \\beta_0 + \\beta_1 t_i + \\beta_2 s_j \\epsilon_{ij} \\] \\[ \\epsilon_{ij} \\sim N(0, \\sigma^2 \\textbf{I}) \\] \\(s_j\\) pertains to the “HUC” levels Think of it as a categorical predictor Goes to \\(0\\) based on which HUC is present m2 = lm(UOM ~ Year + as.factor(huc08), data = mod_df) summary(m2) ## ## Call: ## lm(formula = UOM ~ Year + as.factor(huc08), data = mod_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.4040 -0.4201 -0.1632 0.1809 11.8143 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 44.617022 3.305166 13.499 &lt; 2e-16 *** ## Year -0.021749 0.001642 -13.242 &lt; 2e-16 *** ## as.factor(huc08)10250017 0.368838 0.072993 5.053 4.51e-07 *** ## as.factor(huc08)10270101 -0.187518 0.075699 -2.477 0.013278 * ## as.factor(huc08)10270102 -0.201293 0.068389 -2.943 0.003263 ** ## as.factor(huc08)10270103 -0.214657 0.081542 -2.632 0.008504 ** ## as.factor(huc08)10270104 0.004271 0.068111 0.063 0.950001 ## as.factor(huc08)10270205 0.430969 0.074249 5.804 6.88e-09 *** ## as.factor(huc08)10270207 0.292476 0.077379 3.780 0.000159 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8216 on 4754 degrees of freedom ## Multiple R-squared: 0.108, Adjusted R-squared: 0.1065 ## F-statistic: 71.98 on 8 and 4754 DF, p-value: &lt; 2.2e-16 Where are we headed? 6.2 Stringing sentences Correlative inference Which cruise liner should you avoid? \\[ \\left[y|\\alpha, \\beta \\right] \\equiv \\text{Beta}(\\alpha,\\beta) \\] \\[ \\alpha = \\mu \\phi \\qquad \\beta = (1 - \\mu) \\phi \\qquad g(\\mu) = \\textbf{X}^\\prime \\boldsymbol{\\beta} \\] m1 = gam(infected_proportion ~ as.factor(liner) - 1, data = cruise, family = betar(link = &quot;logit&quot;)) Seems like Fred Olsen is a bad idea… Frequency distribution of cruise liners Half of it, that is… Liner Frequency Aida Cruises 3 Azamara Club Cruises 1 Carnival Cruise Line 2 Celebrity Cruises 4 Crystal Cruises 1 Cunard Line 1 Disney Cruise Line 1 Fred Olsen Cruises 1 Holland America Line 12 Norwegian Cruise Line 3 6.3 Telling stories What just happened? Are these results a problem? Is the context behind them bad? What is data? What is a statistic? What is it for? What are the limitations of statistics? Of science? A major step in telling a story Having an idea for the plot What makes a good plot? How do scientific inquiries happen? By accident (Grokking) By madness (Game theory) Taking risks (Chemotherapy) Via spite (Scale-free networks) “Things breaking” (?) Is Fred Olsen a potent vector of norovirus? Are Carnival cruises really that safe of an option? bbreg = function(){ # process model for(i in 1:n){ y[i] ~ dbeta(alpha[i], beta[i]) # mean/precision parameterization alpha[i] &lt;- mu[i] * phi beta[i] &lt;- (1-mu[i]) * phi # logit link function logit(mu[i]) &lt;- a + b*x[i] } # priors phi ~ dgamma(1,.01) a ~ dnorm(0,.1) b ~ dnorm(0,.1) } # setting up jags data jags_dat = list(y = cruise$infected_proportion, x = as.integer(factor(cruise$liner)), n = nrow(cruise)) # initial proposal values inits = list(a = .01, b = .01, phi = 1) # fitting the model bbreg_mcmc = jags.fit(jags_dat,c(&quot;a&quot;,&quot;b&quot;,&quot;phi&quot;), bbreg,inits,n.adapt=5000, n.update=5000,n.iter=100000, thin=1,n.chains=1) # reproducibility seed set.seed(73) # number of posterior samples K = nrow(samp) # set to carnival and fred olsen x_pred = c(3,8) a_post = samp[,1] # intercept post b_post = samp[,2] # effect par post phi_post = samp[,3] # precision par post # vectorized posterior logit(mu) predictions eta_post = outer(a_post, x_pred, FUN = function(ai, x) ai) + outer(b_post, x_pred) # &quot;un-do&quot; the link function mu_post = invlogit(eta_post) # alpha posterior predictions alpha_post = mu_post * phi_post # beta posterior predictions beta_post = (1 - mu_post) * phi_post # posterior predictions of y y_post = matrix(rbeta(length(alpha_post), alpha_post, beta_post), nrow = K, ncol = length(x_pred)) Any questions? 6.4 Linear models We learned the alphabet Developed our vocabulary And by the end of STAT 240, we formed sentences But do we know what those sentences meant? How can we develop a story if we’re just throwing things together? We need to understand the logic of our phrases The best way to do that is to look at something with well-defined logic In a writing course: Of Mice and Men To Kill a Mockingbird Frankenstein Theater Oedipus Hamlet MacBeth History Fall of the Roman Empire Holy Crusades Transition between WW1 to WW2 For statistics, we use linear models \\[ \\boldsymbol{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k + \\boldsymbol{\\epsilon} \\] \\[ \\boldsymbol{y} = \\textbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \\] Why? We know (almost) everything about them Easy to “extend” the theory to other models “Linear” has a different definition here We’ll stick to “linear in the parameters” Supplemental material will include a more robust definition \\[ \\begin{aligned} &amp; \\boldsymbol{y} = \\beta_0 + \\beta_1 \\boldsymbol{x}_1 + \\beta_2 \\boldsymbol{x}_1^2 + \\boldsymbol{\\epsilon} \\\\ \\\\ &amp; \\boldsymbol{y} = \\beta_0 + \\beta_1 10^x + \\boldsymbol{\\epsilon} \\\\ \\\\ &amp; \\boldsymbol{y} = \\beta_0 + \\beta_1 g(\\boldsymbol{x}_1) + \\beta_2 \\log(\\boldsymbol{x}_2) + \\boldsymbol{\\epsilon}\\\\ \\\\ &amp; \\boldsymbol{y} = \\beta_0 + e^{\\beta_1 \\boldsymbol{x_1}} + \\boldsymbol{\\epsilon}\\\\ \\end{aligned} \\] Which one of these models is linear? What’s a common nonlinear model? \\[ P(t) = P_0 e^{rt} \\] 6.5 Parameter estimation Estimating \\(\\boldsymbol{\\beta}\\) \\[ (\\boldsymbol{X}^\\prime \\boldsymbol{X})^{-1} \\boldsymbol{X}^\\prime \\boldsymbol{y} \\] \\[ \\underset{i}{\\arg\\min} \\sum_{i=1}^n (\\boldsymbol{y}_i - \\boldsymbol{x}_i^\\prime \\boldsymbol{\\beta})^2 \\] \\[ \\underset{i}{\\arg\\max} \\mathcal{L}(\\boldsymbol{y}|\\boldsymbol{\\beta},\\sigma^2) \\] In R heart = read.csv(&quot;data/heart.csv&quot;) y = heart$chol x1 = heart$age Ordinary least squares X = cbind(1,x1) bhat = solve(t(X)%*%X)%*%t(X)%*%y bhat ## [,1] ## 179.967471 ## x1 1.219441 Optimization optim(par=c(0,0),method = c(&quot;Nelder-Mead&quot;),fn=function(beta){sum((y-(beta[1]+beta[2]*x1))^2)}) ## $par ## [1] 180.014143 1.218704 ## ## $value ## [1] 774258.3 ## ## $counts ## function gradient ## 111 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL Using R’s lm() method lm(y ~ x1) ## ## Call: ## lm(formula = y ~ x1) ## ## Coefficients: ## (Intercept) x1 ## 179.967 1.219 Maximum Likelihood Estimation nll = function(par){ beta = par[1:2] sig2 = par[3] -sum(dnorm(y,X%*%beta,sqrt(sig2),log=TRUE)) } optim(par=c(0,0,2),fn=nll,method = &quot;Nelder-Mead&quot;) ## $par ## [1] 180.513781 1.210868 2555.099421 ## ## $value ## [1] 1618.597 ## ## $counts ## function gradient ## 260 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL lm() summary(lm(y ~ x1)) ## ## Call: ## lm(formula = y ~ x1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -123.476 -32.560 -5.745 28.024 302.330 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 179.9675 17.7116 10.161 &lt; 2e-16 *** ## x1 1.2194 0.3213 3.795 0.000179 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 50.72 on 301 degrees of freedom ## Multiple R-squared: 0.04566, Adjusted R-squared: 0.04249 ## F-statistic: 14.4 on 1 and 301 DF, p-value: 0.0001786 6.6 Live example R code 6.7 The path forward Linear model theory and application Hypothesis testing of parameters Building models Assignment 2 Assignment 3 6.8 Go away "],["day-7.html", "7 Day 7 7.1 Review 7.2 Multivariate Normal Distributions 7.3 The simplest model 7.4 Live example 7.5 Go away", " 7 Day 7 7.1 Review Linear model \\[ \\boldsymbol{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k + \\boldsymbol{\\epsilon} \\] \\[ \\boldsymbol{y} = \\textbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \\] Estimating \\(\\boldsymbol{\\beta}\\) \\[ (\\boldsymbol{X}^\\prime \\boldsymbol{X})^{-1} \\boldsymbol{X}^\\prime \\boldsymbol{y} \\] \\[ \\underset{i}{\\arg\\min} \\sum_{i=1}^n (\\boldsymbol{y}_i - \\boldsymbol{x}_i^\\prime \\boldsymbol{\\beta})^2 \\] \\[ \\underset{i}{\\arg\\max} \\mathcal{L}(\\boldsymbol{y}|\\boldsymbol{\\beta},\\sigma^2) \\] Ordinary least squares heart = read.csv(&quot;data/heart.csv&quot;) y = heart$chol x1 = heart$age X = cbind(1,x1) bhat = solve(t(X)%*%X)%*%t(X)%*%y bhat ## [,1] ## 179.967471 ## x1 1.219441 Using R’s lm() method lm(y ~ x1) ## ## Call: ## lm(formula = y ~ x1) ## ## Coefficients: ## (Intercept) x1 ## 179.967 1.219 summary(lm(y ~ x)) summary(lm(y ~ x1)) ## ## Call: ## lm(formula = y ~ x1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -123.476 -32.560 -5.745 28.024 302.330 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 179.9675 17.7116 10.161 &lt; 2e-16 *** ## x1 1.2194 0.3213 3.795 0.000179 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 50.72 on 301 degrees of freedom ## Multiple R-squared: 0.04566, Adjusted R-squared: 0.04249 ## F-statistic: 14.4 on 1 and 301 DF, p-value: 0.0001786 Any questions? 7.2 Multivariate Normal Distributions Your (brief) introduction to math stats STAT 510/610/770 will do it better No hate mail \\[ \\boldsymbol{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{bmatrix} \\sim N\\left(\\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\end{bmatrix}, \\begin{bmatrix} \\sigma^2 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma^2 \\\\ \\end{bmatrix}\\right) \\] \\[ \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\end{bmatrix} = \\boldsymbol{\\mu} \\] \\[ \\begin{bmatrix} \\sigma^2 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma^2 \\\\ \\end{bmatrix} = \\sigma^2 \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ \\end{bmatrix} = \\sigma^2 \\textbf{I} \\] \\[ \\boldsymbol{y} \\sim N(\\boldsymbol{\\mu},\\sigma^2 \\textbf{I}) \\] “i.i.d.”: independent and identically distributed Sometimes referred to as a random sample Assume that all observations of \\(y\\) arise from the same mean and variance Covariance \\[ \\text{Cov}(X,Y) = \\mathbb{E}\\left[(X - \\mathbb{E}X)(Y - \\mathbb{E}Y) \\right] = \\mathbb{E}XY - \\mathbb{E}X \\mathbb{E}Y \\] # covariance between two vectors x = runif(3) y = rnorm(3) cov(x,y) ## [1] -0.1246672 \\[ \\boldsymbol{y} \\sim N(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}) \\] \\[ \\boldsymbol{\\Sigma} = \\begin{bmatrix} \\sigma_{y_{11}} &amp; \\sigma_{y_{12}} &amp; ... \\sigma_{y_{1p}} \\\\ \\sigma_{y_{21}} &amp; \\sigma_{y_{22}} &amp; ... \\sigma_{y_{2p}} \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ \\sigma_{y_{n1}} &amp; \\sigma_{y_{n2}} ... &amp; \\sigma_{y_{np}} \\end{bmatrix} \\] \\[ \\sigma_{y_{11}} = \\sigma^2_{y_1} \\] # covariance of a matrix x = cbind(runif(3),runif(3),runif(3)) cov(x) ## [,1] [,2] [,3] ## [1,] 0.116413718 0.12195714 0.002065897 ## [2,] 0.121957145 0.16299819 -0.017917065 ## [3,] 0.002065897 -0.01791707 0.011481972 Joint distributions \\[ y_1,y_2,...,y_n \\overset{\\text{iid}}{\\sim} N(\\mu,\\sigma^2) \\] Think of the multiplication rule for independent events \\[ f_{X,Y}(x,y) = N(\\mu_{x,y},\\sigma^2_{x,y}) \\] Conditional distributions \\[ f_{Y|X}(y|x) = \\frac{f_{X,Y}(x,y)}{f_{Y}(y)} \\] “Gelman” notation \\[ \\left[ y|x \\right] = \\frac{\\left[ x,y \\right]}{\\left[ y \\right]} \\] 7.3 The simplest model ambient = read.csv(&quot;data/ambient_data_full.csv&quot;, stringsAsFactors = FALSE) ambient = ambient[,c(2:14)] # pull only the desired elements # subset for only specific pollutants amb = subset(ambient, ambient$pollute %in% c(&quot;Ammonia&quot;, &quot;Atrizine&quot;, &quot;Chloride&quot;, &quot;Nitrate&quot;, &quot;Nitrite&quot;, &quot;NO2NO3&quot;, &quot;Phosphorus&quot;, &quot;TDS&quot;, &quot;TKN&quot;)) head(amb) ## Basin Site Loct Date Time UOM Year Month Day ## 32550 SMOKY-SALINE RIVER BASIN SC007 Storet Transfer Jan. 19, 2009 1975-11-19 1210 0.22 1975 11 19 ## 32551 SMOKY-SALINE RIVER BASIN SC007 Storet Transfer Jan. 19, 2009 1975-12-17 950 0.21 1975 12 17 ## 32552 SMOKY-SALINE RIVER BASIN SC007 Storet Transfer Jan. 19, 2009 1976-01-14 1000 0.10 1976 1 14 ## 32553 SMOKY-SALINE RIVER BASIN SC007 Storet Transfer Jan. 19, 2009 1976-02-18 1000 0.15 1976 2 18 ## 32554 SMOKY-SALINE RIVER BASIN SC007 Storet Transfer Jan. 19, 2009 1976-03-17 930 0.03 1976 3 17 ## 32555 SMOKY-SALINE RIVER BASIN SC007 Storet Transfer Jan. 19, 2009 1976-05-19 1000 0.16 1976 5 19 ## Lat Long Huc8 pollute ## 32550 38.77674 -98.85472 10260006 Ammonia ## 32551 38.77674 -98.85472 10260006 Ammonia ## 32552 38.77674 -98.85472 10260006 Ammonia ## 32553 38.77674 -98.85472 10260006 Ammonia ## 32554 38.77674 -98.85472 10260006 Ammonia ## 32555 38.77674 -98.85472 10260006 Ammonia # rarely is ggplot2 strictly better at something library(ggplot2) # this is one of those rare cases # 3x3 scatterplot ggplot(amb, aes(Year, UOM)) + # define plot object # scatter plot with small points and transparent colors geom_point(size = 0.6, color = &quot;#00000040&quot;) + # set x and y axis labels labs(x = &quot;Year&quot;, y = &quot;Measurement (mg/L)&quot;) + theme_minimal() + # simple theme # remove background panels all ggplots default to theme(panel.grid.minor = element_blank(), panel.grid.major = element_blank()) + # plot by groups with individual y axis scales facet_wrap(~ pollute, scales = &quot;free_y&quot;) # we&#39;ll just look at phosphorus phos = subset(amb, amb$pollute == &quot;Phosphorus&quot;) par(mar = c(4.5,4.5,1,1)) plot(phos$Year, phos$UOM, col = &quot;#00000050&quot;, pch = 20, cex = 0.9, xlab = &quot;Year&quot;, ylab = &quot;Measured Amount (mg/L)&quot;) Propose a model What does this mean? (Whiteboard) Intercept-only \\[ \\boldsymbol{y} = \\beta_0 + \\boldsymbol{\\epsilon} \\] # year measure was taken x = phos$Year # pollutant measurements y = phos$UOM # intercept only model m1 = lm(y ~ 1) coef(m1) ## (Intercept) ## 0.3470304 A few fun ways to look at this: \\[ \\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1} \\bar{x} \\] \\[ \\underset{i}{\\arg\\min} \\sum_{i=1}^n (y_i - \\beta x)^2 \\] \\[ \\underset{i}{\\arg\\max} \\mathcal{L}(y|\\beta_0,\\sigma^2) \\] An even more amusing one; what is the structure of \\(\\boldsymbol{X}\\) here? \\[ \\boldsymbol{y} = \\boldsymbol{X \\beta} + \\boldsymbol{\\epsilon} \\] n = length(y) # sample size of y n ## [1] 46045 j = matrix(1,n,1,T) # j vector # j&#39;j = n t(j)%*%j ## [,1] ## [1,] 46045 # (j&#39;j)^-1 = 1/n solve(t(j)%*%j) ## [,1] ## [1,] 2.171788e-05 # j&#39;y = sum(y) t(j)%*%y ## [,1] ## [1,] 15979.02 sum(y) ## [1] 15979.02 # all together solve(t(j)%*%j)%*%t(j)%*%y ## [,1] ## [1,] 0.3470304 # in a simpler form (y%*%j)/n ## [,1] ## [1,] 0.3470304 mean(y) ## [1] 0.3470304 par(mar = c(4.5,4.5,1,1)) plot(x, y, col = &quot;#00000050&quot;, pch = 20, cex = 0.9, xlab = &quot;Year&quot;, ylab = &quot;Measured Amount (mg/L)&quot;) abline(h = mean(y), col = &quot;gold&quot;, lwd = 3) Simple linear model \\[ \\boldsymbol{y} = \\beta_0 + \\beta_1 \\boldsymbol{x} + \\boldsymbol{\\epsilon} \\] \\[ \\mathbb{E}(\\boldsymbol{\\epsilon}) = 0 \\] \\[ \\mathbb{E}(\\boldsymbol{\\epsilon}) = \\boldsymbol{y} - \\hat{\\boldsymbol{y}} = \\boldsymbol{y} - (\\beta_0 + \\beta_1 \\boldsymbol{x}) = 0 \\] What is \\(\\hat{\\boldsymbol{y}}\\)? The prediction of \\(\\mathbb{E}(\\boldsymbol{y})\\) \\[ \\boldsymbol{y} \\sim N(\\beta_0 + \\beta_1 \\boldsymbol{x}, \\sigma^2 \\textbf{I}) \\] \\(\\mathbb{E}(\\boldsymbol{y}) = \\boldsymbol{\\mu}\\) \\(\\hat{\\boldsymbol{y}} = \\hat{\\boldsymbol{\\mu}} = \\widehat{\\mathbb{E}(\\boldsymbol{y})}\\) When we estimate \\(\\hat{\\boldsymbol{\\beta}}\\) We’re predicting an expected value of an unknown population parameter Think about assignment 2 question 11 Goal: propose a model that represents the data generating process of \\(\\boldsymbol{y}\\) Ideal: capture the entire process Reality: \\(\\approx 50\\%\\) most of the time \\[ \\boldsymbol{y} = \\beta_0 + \\beta_1 \\boldsymbol{t} + \\boldsymbol{\\epsilon} \\] # simple linear regression m2 = lm(y ~ x) # time as sole predictor summary(m2) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.5494 -0.2506 -0.1425 0.0356 18.5063 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.0928188 0.4103373 34.34 &lt;2e-16 *** ## x -0.0068752 0.0002052 -33.50 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6084 on 46043 degrees of freedom ## Multiple R-squared: 0.02379, Adjusted R-squared: 0.02377 ## F-statistic: 1122 on 1 and 46043 DF, p-value: &lt; 2.2e-16 par(mar = c(4.5,4.5,1,1)) plot(x, y, col = &quot;#00000050&quot;, pch = 20, cex = 0.9, xlab = &quot;Year&quot;, ylab = &quot;Measured Amount (mg/L)&quot;) abline(h = mean(y), col = &quot;gold&quot;, lwd = 3) abline(a = coef(m2)[1], b = coef(m2)[2], col = &quot;red&quot;, lwd = 3) Why do we use an intercept only model? How much better is model 2 versus model 1? Which are you choosing if you had to? 7.4 Live example R code 7.5 Go away "],["day-8.html", "8 Day 8 8.1 Review 8.2 Adding complexity 8.3 Model development 8.4 Confidence intervals 8.5 Derived quantities 8.6 R programming Lab 8.7 Go away", " 8 Day 8 8.1 Review Multivariate Normal Distributions \\[ \\boldsymbol{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{bmatrix} \\sim N\\left(\\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\end{bmatrix}, \\begin{bmatrix} \\sigma^2 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma^2 \\\\ \\end{bmatrix}\\right) \\] \\[ \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\end{bmatrix} = \\boldsymbol{\\mu} \\] \\[ \\begin{bmatrix} \\sigma^2 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma^2 \\\\ \\end{bmatrix} = \\sigma^2 \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ \\end{bmatrix} = \\sigma^2 \\textbf{I} \\] \\[ \\boldsymbol{y} \\sim N(\\boldsymbol{\\mu},\\sigma^2 \\textbf{I}) \\] “i.i.d.”: independent and identically distributed Sometimes referred to as a random sample Assume that all observations of \\(y\\) arise from the same mean and variance Covariance \\[ \\text{Cov}(X,Y) = \\mathbb{E}\\left[(X - \\mathbb{E}X)(Y - \\mathbb{E}Y) \\right] = \\mathbb{E}XY - \\mathbb{E}X \\mathbb{E}Y \\] # covariance between two vectors x = runif(3) y = rnorm(3) cov(x,y) ## [1] 0.0694238 \\[ \\boldsymbol{y} \\sim N(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}) \\] \\[ \\boldsymbol{\\Sigma} = \\begin{bmatrix} \\sigma_{y_{11}} &amp; \\sigma_{y_{12}} &amp; ... \\sigma_{y_{1p}} \\\\ \\sigma_{y_{21}} &amp; \\sigma_{y_{22}} &amp; ... \\sigma_{y_{2p}} \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ \\sigma_{y_{n1}} &amp; \\sigma_{y_{n2}} ... &amp; \\sigma_{y_{np}} \\end{bmatrix} \\] \\[ \\sigma_{y_{11}} = \\sigma^2_{y_1} \\] # covariance of a matrix x = cbind(runif(3),runif(3),runif(3)) cov(x) ## [,1] [,2] [,3] ## [1,] 0.03229371 -0.02508852 0.03336786 ## [2,] -0.02508852 0.06556879 0.04348240 ## [3,] 0.03336786 0.04348240 0.13902055 Joint distributions \\[ y_1,y_2,...,y_n \\overset{\\text{iid}}{\\sim} N(\\mu,\\sigma^2) \\] Think of the multiplication rule for independent events \\[ f_{X,Y}(x,y) = N(\\mu_{x,y},\\sigma^2_{x,y}) \\] Conditional distributions \\[ f_{Y|X}(y|x) = \\frac{f_{X,Y}(x,y)}{f_{Y}(y)} \\] “Gelman” notation \\[ \\left[ y|x \\right] = \\frac{\\left[ x,y \\right]}{\\left[ y \\right]} \\] Intercept-only \\[ \\boldsymbol{y} = \\beta_0 + \\boldsymbol{\\epsilon} \\] # year measure was taken x = phos$Year # pollutant measurements y = phos$UOM # intercept only model m1 = lm(y ~ 1) coef(m1) ## (Intercept) ## 0.3470304 What is the structure of \\(\\boldsymbol{X}\\) here? \\[ \\boldsymbol{y} = \\boldsymbol{X \\beta} + \\boldsymbol{\\epsilon} \\] n = length(y) # sample size of y n ## [1] 46045 j = matrix(1,n,1,T) # j vector # j&#39;j = n t(j)%*%j ## [,1] ## [1,] 46045 # (j&#39;j)^-1 = 1/n solve(t(j)%*%j) ## [,1] ## [1,] 2.171788e-05 # j&#39;y = sum(y) t(j)%*%y ## [,1] ## [1,] 15979.02 sum(y) ## [1] 15979.02 # all together solve(t(j)%*%j)%*%t(j)%*%y ## [,1] ## [1,] 0.3470304 # in a simpler form (y%*%j)/n ## [,1] ## [1,] 0.3470304 mean(y) ## [1] 0.3470304 Simple linear model \\[ \\boldsymbol{y} = \\beta_0 + \\beta_1 \\boldsymbol{x} + \\boldsymbol{\\epsilon} \\] \\[ \\mathbb{E}(\\boldsymbol{\\epsilon}) = 0 \\] \\[ \\mathbb{E}(\\boldsymbol{\\epsilon}) = \\boldsymbol{y} - \\hat{\\boldsymbol{y}} = \\boldsymbol{y} - (\\beta_0 + \\beta_1 \\boldsymbol{x}) = 0 \\] \\(\\mathbb{E}(\\boldsymbol{y}) = \\boldsymbol{\\mu}\\) \\(\\hat{\\boldsymbol{y}} = \\hat{\\boldsymbol{\\mu}} = \\widehat{\\mathbb{E}(\\boldsymbol{y})}\\) Predicting an expected value of an unknown population parameter Goal: propose a model that represents the data generating process of \\(\\boldsymbol{y}\\) Ideal: capture the entire process Reality: \\(\\approx 50\\%\\) most of the time \\[ \\boldsymbol{y} = \\beta_0 + \\beta_1 \\boldsymbol{t} + \\boldsymbol{\\epsilon} \\] # simple linear regression m2 = lm(y ~ x) # time as sole predictor summary(m2) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.5494 -0.2506 -0.1425 0.0356 18.5063 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.0928188 0.4103373 34.34 &lt;2e-16 *** ## x -0.0068752 0.0002052 -33.50 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6084 on 46043 degrees of freedom ## Multiple R-squared: 0.02379, Adjusted R-squared: 0.02377 ## F-statistic: 1122 on 1 and 46043 DF, p-value: &lt; 2.2e-16 par(mar = c(4.5,4.5,1,1)) plot(x, y, col = &quot;#00000050&quot;, pch = 20, cex = 0.9, xlab = &quot;Year&quot;, ylab = &quot;Measured Amount (mg/L)&quot;) abline(h = mean(y), col = &quot;gold&quot;, lwd = 3) abline(a = coef(m2)[1], b = coef(m2)[2], col = &quot;red&quot;, lwd = 3) Why do we use an intercept only model? How much better is model 2 versus model 1? Which are you choosing if you had to? Any questions? 8.2 Adding complexity Multiple linear regression \\[ \\boldsymbol{y} = \\beta_0 + \\beta_1 \\boldsymbol{x}_1 + \\beta_2 \\boldsymbol{x}_2 + ... + \\beta_k \\boldsymbol{x}_k + \\boldsymbol{\\epsilon} \\] Problems What does \\(\\beta_0\\) represent? \\(\\beta_1\\)? \\(\\beta_2\\)? \\[ \\boldsymbol{y} = \\boldsymbol{X \\beta} + \\boldsymbol{\\epsilon} \\] \\[ \\widehat{\\mathbb{E}(\\boldsymbol{y})} = \\hat{\\boldsymbol{\\beta}} = (\\boldsymbol{X}^\\prime \\boldsymbol{X})^{-1} \\boldsymbol{X}^\\prime \\boldsymbol{y} \\] Geometry (whiteboard) How can we conceptualize so many dimensions? How do we explain the parameters? Predict? We cannot simply assume that the coefficients are the sum of all their simple regressions. 8.3 Model development # framingham heart study (long term cohort study on heart health) framingham = read.csv(&quot;data/framingham_heart_study.csv&quot;) # reduce dimensions and throw out NA values df = na.omit(framingham[,c(1,2,5,10:15)]) # first 6 rows of each column head(df) ## male age cigsPerDay totChol sysBP diaBP BMI heartRate glucose ## 1 1 39 0 195 106.0 70 26.97 80 77 ## 2 0 46 0 250 121.0 81 28.73 95 76 ## 3 1 48 20 245 127.5 80 25.34 75 70 ## 4 0 61 30 225 150.0 95 28.58 65 103 ## 5 0 46 23 285 130.0 84 23.10 85 85 ## 6 0 43 0 228 180.0 110 30.30 77 99 # correlation between each variable and BMI cor(df)[7,] ## male age cigsPerDay totChol sysBP diaBP BMI heartRate ## 0.07608980 0.13639588 -0.09296323 0.11436650 0.32772352 0.38052898 1.00000000 0.07141530 ## glucose ## 0.08882778 Propose a model for BMI (whiteboard) \\[ \\boldsymbol{y} = \\beta_0 + \\beta_1 \\boldsymbol{x}_1 + \\boldsymbol{\\epsilon} \\] sysBP \\[ \\boldsymbol{y} = \\beta_0 + \\beta_1 \\boldsymbol{x}_1 + \\beta_2 \\boldsymbol{x}_2 + \\boldsymbol{\\epsilon} \\] diaBP \\[ \\boldsymbol{y} = \\beta_0 + \\beta_1 \\boldsymbol{x}_1 + \\beta_2 \\boldsymbol{x}_2 + \\beta_3 \\boldsymbol{x}_3 + \\boldsymbol{\\epsilon} \\] age \\[ \\boldsymbol{y} = \\beta_0 + \\beta_1 \\boldsymbol{x}_1 + \\beta_2 \\boldsymbol{x}_2 + \\beta_3 \\boldsymbol{x}_3 + \\beta_4 \\boldsymbol{x}_4 + \\boldsymbol{\\epsilon} \\] cigsPerDay m1 = lm(BMI ~ sysBP, data = df) m2 = lm(BMI ~ sysBP + diaBP, data = df) m3 = lm(BMI ~ sysBP + diaBP + age, data = df) m4 = lm(BMI ~ sysBP + diaBP + age + cigsPerDay, data = df) # matrix for tabling coefficient results cmat = matrix(c(mean(df$BMI), 0, 0, 0, 0, coef(m1)[1], coef(m1)[2], 0, 0, 0, coef(m2)[1], coef(m2)[2], coef(m2)[3], 0, 0, coef(m3)[1], coef(m3)[2], coef(m3)[3], coef(m3)[4], 0, coef(m4)[1], coef(m4)[2], coef(m4)[3], coef(m4)[4], coef(m4)[5]), 5,5,T) colnames(cmat) = c(&quot;b0&quot;, &quot;b1&quot;, &quot;b2&quot;, &quot;b3&quot;, &quot;b4&quot;) rownames(cmat) = c(&quot;IO&quot;, &quot;M1&quot;, &quot;M2&quot;, &quot;M3&quot;, &quot;M4&quot;) cmat ## b0 b1 b2 b3 b4 ## IO 25.80483 0.000000000 0.0000000 0.00000000 0.00000000 ## M1 17.81498 0.060324144 0.0000000 0.00000000 0.00000000 ## M2 14.90431 0.013900737 0.1092114 0.00000000 0.00000000 ## M3 14.07433 0.008316134 0.1138074 0.02394858 0.00000000 ## M4 14.56847 0.008139557 0.1136106 0.01859131 -0.02095906 What’s happening as we add predictors? Are certain predictors “better” than others? par(mfrow = c(1, 2), mar = c(4.5, 1, 1, 1), oma = c(0, 4, 0, 0)) plot(df$sysBP, df$BMI, xlab = &quot;Systolic&quot;, ylab = &quot;&quot;, pch = 20, col = &quot;#00000050&quot;) plot(df$diaBP, df$BMI, xlab = &quot;Diastolic&quot;, ylab = &quot;&quot;, yaxt = &quot;n&quot;, pch = 20, col = &quot;#00000050&quot;) mtext(&quot;BMI&quot;, side = 2, outer = TRUE, line = 2) par(mar = c(4.5,4.5,1,1)) plot(df$sysBP,df$diaBP, xlab = &quot;Systolic&quot;, ylab = &quot;Diastolic&quot;, pch = 20, col = &quot;#00000050&quot;) 8.4 Confidence intervals Ogallala aquifer Major concern for KS/TX/NE Drying up # historic water level data 1995-2013 wlev = read.csv(&quot;data/ext_data/KS_Water_Level_Monitoring_95to13.csv&quot;, stringsAsFactors = F) wlev = wlev[,-1] # remove index column plot(wlev$day_i,wlev$lev_va_ft, xlab = &quot;Days since 11-01-94&quot;, ylab = &quot;Water level (ft)&quot;, pch = 20, col = &quot;#00000030&quot;) m1 = lm(lev_va_ft ~ day_i, data = wlev) summary(m1) ## ## Call: ## lm(formula = lev_va_ft ~ day_i, data = wlev) ## ## Residuals: ## Min 1Q Median 3Q Max ## -95.15 -62.72 -35.56 53.98 336.07 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 97.2558228 0.9746107 99.79 &lt;2e-16 *** ## day_i -0.0037165 0.0002375 -15.65 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 77.54 on 27469 degrees of freedom ## Multiple R-squared: 0.008838, Adjusted R-squared: 0.008802 ## F-statistic: 244.9 on 1 and 27469 DF, p-value: &lt; 2.2e-16 There’s usually three ways to work with a problem in R Throw math at it # degrees of freedom dfr = nrow(wlev) - 2 # t value t_v = qt(0.975,dfr) # standard error of beta 1 stde_b1 = diag(vcov(m1))[2]^0.5 coef(m1)[2] - t_v*stde_b1 # lower ## day_i ## -0.004181951 coef(m1)[2] + t_v*stde_b1 # upper ## day_i ## -0.003251031 Throw definitions / structures at it # standard error of beta 1 summary(m1)$coefficients[,2][2] ## day_i ## 0.0002374733 stde_b1 ## day_i ## 0.0002374733 # very close to z at 95% t_v ## [1] 1.96005 # upper and lower ci coef(m1)[2] - 1.96 * summary(m1)$coefficients[,2][2] ## day_i ## -0.004181939 coef(m1)[2] + 1.96 * summary(m1)$coefficients[,2][2] ## day_i ## -0.003251043 Throw functions / packages at it # base R function for ci confint(m1)[2,] ## 2.5 % 97.5 % ## -0.004181951 -0.003251031 Interpreting confidence intervals Misconceptions How to lie with statistics set.seed(73) # predictions with confidence bands pred = predict(m1, type = &quot;response&quot;, interval = &quot;confidence&quot;) # plot including conf bands or = order(wlev$day_i) plot(wlev$day_i,wlev$lev_va_ft, xlab = &quot;Days since 11-01-94&quot;, ylab = &quot;Water level (ft)&quot;, pch = 20, col = &quot;#D1D1D170&quot;) polygon(c(wlev$day_i[or], rev(wlev$day_i[or])), c(pred[,2][or], rev(pred[,3][or])), col = &quot;#51288570&quot;, border = NA) lines(wlev$day_i[or], pred[,1][or], lwd = 2, col = &quot;#512885&quot;) plot(wlev$day_i,wlev$lev_va_ft, xlab = &quot;Days since 11-01-94&quot;, ylab = &quot;Water level (ft)&quot;, pch = 20, col = &quot;#D1D1D170&quot;, ylim = c(70,100)) polygon(c(wlev$day_i[or], rev(wlev$day_i[or])), c(pred[,2][or], rev(pred[,3][or])), col = &quot;#51288570&quot;, border = NA) lines(wlev$day_i[or], pred[,1][or], lwd = 2, col = &quot;#512885&quot;) 8.5 Derived quantities When will the Ogallala aquifer dry up? Probably a question we should ask Not an answer that’s nice to think about # solution for x intercept dry_day = as.numeric(-coef(m1)[1]/coef(m1)[2]) dry_day # the day the aquifer will dry up ## [1] 26168.72 dry_year = dry_day/365 dry_year # year aquifer dries up ## [1] 71.69513 # position at nov 1 1994 dry_year + 1994.917 ## [1] 2066.612 8.6 R programming Lab Read these two pages from [Wood (2006) # install.packages(&quot;gamair&quot;) library(gamair) data(hubble) par(mar = c(4.5,4.5,2,1)) plot(hubble$x,hubble$y, xlab = &quot;Distance in Mega parsecs&quot;, ylab = &quot;Velocity (km/s)&quot;, pch = 19) m = lm(hubble$y ~ hubble$x - 1) abline(m) Using the hubble data in the gamair package: Write out a linear model for predicting the velocity of the Cepheid stars observed by the Hubble space telescope based on their distance in mega parsecs, based on Hubble’s law. State and briefly explain what the assumptions behind that model are. Fit the data in R using the matrix form of method of least squares. Save the results to a variable in R. \\[ \\boldsymbol{\\beta} = (\\boldsymbol{X}^\\prime \\boldsymbol{X})^{-1} \\boldsymbol{X}^\\prime y \\] Fit the data in R using the lm() function. Save the model. Compare the coefficients from (3) and (4). Are they different? Add the fitted regression line from (4) to the plot of the original data. Set the line width to 2 and change the color to be different from the points. Compute the confidence interval for the coefficients in (4) using any method you prefer. Save them. Based off of the reading, predict the age of the universe. Note that the distance is measured in Mega parsecs, which are \\(3.09 \\times 10^19\\) km. 8.7 Go away "],["day-9.html", "9 Day 9 9.1 Review 9.2 Confidence intervals for derived quantities 9.3 Non-parametric bootstrap 9.4 Concept demo 9.5 R programming Lab 9.6 Go away 9.7 Extended notes on the Delta Method", " 9 Day 9 9.1 Review Multiple linear regression \\[ \\boldsymbol{y} = \\beta_0 + \\beta_1 \\boldsymbol{x}_1 + \\beta_2 \\boldsymbol{x}_2 + ... + \\beta_k \\boldsymbol{x}_k + \\boldsymbol{\\epsilon} \\] \\[ \\boldsymbol{y} = \\boldsymbol{X \\beta} + \\boldsymbol{\\epsilon} \\] \\[ \\widehat{\\mathbb{E}(\\boldsymbol{y})} = \\hat{\\boldsymbol{\\beta}} = (\\boldsymbol{X}^\\prime \\boldsymbol{X})^{-1} \\boldsymbol{X}^\\prime \\boldsymbol{y} \\] # framingham heart study (long term cohort study on heart health) framingham = read.csv(&quot;data/framingham_heart_study.csv&quot;) # reduce dimensions and throw out NA values df = na.omit(framingham[,c(1,2,5,10:15)]) # first 6 rows of each column head(df) ## male age cigsPerDay totChol sysBP diaBP BMI heartRate glucose ## 1 1 39 0 195 106.0 70 26.97 80 77 ## 2 0 46 0 250 121.0 81 28.73 95 76 ## 3 1 48 20 245 127.5 80 25.34 75 70 ## 4 0 61 30 225 150.0 95 28.58 65 103 ## 5 0 46 23 285 130.0 84 23.10 85 85 ## 6 0 43 0 228 180.0 110 30.30 77 99 # correlation between each variable and BMI cor(df)[7,] ## male age cigsPerDay totChol sysBP diaBP BMI heartRate ## 0.07608980 0.13639588 -0.09296323 0.11436650 0.32772352 0.38052898 1.00000000 0.07141530 ## glucose ## 0.08882778 m1 = lm(BMI ~ sysBP, data = df) m2 = lm(BMI ~ sysBP + diaBP, data = df) m3 = lm(BMI ~ sysBP + diaBP + age, data = df) m4 = lm(BMI ~ sysBP + diaBP + age + cigsPerDay, data = df) # matrix for tabling coefficient results cmat = matrix(c(mean(df$BMI), 0, 0, 0, 0, coef(m1)[1], coef(m1)[2], 0, 0, 0, coef(m2)[1], coef(m2)[2], coef(m2)[3], 0, 0, coef(m3)[1], coef(m3)[2], coef(m3)[3], coef(m3)[4], 0, coef(m4)[1], coef(m4)[2], coef(m4)[3], coef(m4)[4], coef(m4)[5]), 5,5,T) colnames(cmat) = c(&quot;b0&quot;, &quot;b1&quot;, &quot;b2&quot;, &quot;b3&quot;, &quot;b4&quot;) rownames(cmat) = c(&quot;IO&quot;, &quot;M1&quot;, &quot;M2&quot;, &quot;M3&quot;, &quot;M4&quot;) cmat ## b0 b1 b2 b3 b4 ## IO 25.80483 0.000000000 0.0000000 0.00000000 0.00000000 ## M1 17.81498 0.060324144 0.0000000 0.00000000 0.00000000 ## M2 14.90431 0.013900737 0.1092114 0.00000000 0.00000000 ## M3 14.07433 0.008316134 0.1138074 0.02394858 0.00000000 ## M4 14.56847 0.008139557 0.1136106 0.01859131 -0.02095906 What’s happening as we add predictors? Are certain predictors “better” than others? par(mfrow = c(1, 2), mar = c(4.5, 1, 1, 1), oma = c(0, 4, 0, 0)) plot(df$sysBP, df$BMI, xlab = &quot;Systolic&quot;, ylab = &quot;&quot;, pch = 20, col = &quot;#00000050&quot;) plot(df$diaBP, df$BMI, xlab = &quot;Diastolic&quot;, ylab = &quot;&quot;, yaxt = &quot;n&quot;, pch = 20, col = &quot;#00000050&quot;) mtext(&quot;BMI&quot;, side = 2, outer = TRUE, line = 2) par(mar = c(4.5,4.5,1,1)) plot(df$sysBP,df$diaBP, xlab = &quot;Systolic&quot;, ylab = &quot;Diastolic&quot;, pch = 20, col = &quot;#00000050&quot;) Confidence intervals # historic water level data 1995-2013 wlev = read.csv(&quot;data/ext_data/KS_Water_Level_Monitoring_95to13.csv&quot;, stringsAsFactors = F) wlev = wlev[,-1] # remove index column plot(wlev$day_i,wlev$lev_va_ft, xlab = &quot;Days since 11-01-94&quot;, ylab = &quot;Water level (ft)&quot;, pch = 20, col = &quot;#00000030&quot;) m1 = lm(lev_va_ft ~ day_i, data = wlev) summary(m1) ## ## Call: ## lm(formula = lev_va_ft ~ day_i, data = wlev) ## ## Residuals: ## Min 1Q Median 3Q Max ## -95.15 -62.72 -35.56 53.98 336.07 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 97.2558228 0.9746107 99.79 &lt;2e-16 *** ## day_i -0.0037165 0.0002375 -15.65 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 77.54 on 27469 degrees of freedom ## Multiple R-squared: 0.008838, Adjusted R-squared: 0.008802 ## F-statistic: 244.9 on 1 and 27469 DF, p-value: &lt; 2.2e-16 # degrees of freedom dfr = nrow(wlev) - 2 # t value t_v = qt(0.975,dfr) # standard error of beta 1 stde_b1 = diag(vcov(m1))[2]^0.5 coef(m1)[2] - t_v*stde_b1 # lower ## day_i ## -0.004181951 coef(m1)[2] + t_v*stde_b1 # upper ## day_i ## -0.003251031 Throw definitions / structures at it # standard error of beta 1 summary(m1)$coefficients[,2][2] ## day_i ## 0.0002374733 stde_b1 ## day_i ## 0.0002374733 # very close to z at 95% t_v ## [1] 1.96005 # upper and lower ci coef(m1)[2] - 1.96 * summary(m1)$coefficients[,2][2] ## day_i ## -0.004181939 coef(m1)[2] + 1.96 * summary(m1)$coefficients[,2][2] ## day_i ## -0.003251043 Throw functions / packages at it # base R function for ci confint(m1)[2,] ## 2.5 % 97.5 % ## -0.004181951 -0.003251031 Interpreting confidence intervals # predictions with confidence bands pred = predict(m1, type = &quot;response&quot;, interval = &quot;confidence&quot;) # plot including conf bands or = order(wlev$day_i) plot(wlev$day_i,wlev$lev_va_ft, xlab = &quot;Days since 11-01-94&quot;, ylab = &quot;Water level (ft)&quot;, pch = 20, col = &quot;#D1D1D170&quot;) polygon(c(wlev$day_i[or], rev(wlev$day_i[or])), c(pred[,2][or], rev(pred[,3][or])), col = &quot;#51288570&quot;, border = NA) lines(wlev$day_i[or], pred[,1][or], lwd = 2, col = &quot;#512885&quot;) plot(wlev$day_i,wlev$lev_va_ft, xlab = &quot;Days since 11-01-94&quot;, ylab = &quot;Water level (ft)&quot;, pch = 20, col = &quot;#D1D1D170&quot;, ylim = c(70,100)) polygon(c(wlev$day_i[or], rev(wlev$day_i[or])), c(pred[,2][or], rev(pred[,3][or])), col = &quot;#51288570&quot;, border = NA) lines(wlev$day_i[or], pred[,1][or], lwd = 2, col = &quot;#512885&quot;) Derived quantities # solution for x intercept dry_day = as.numeric(-coef(m1)[1]/coef(m1)[2]) dry_day # the day the aquifer will dry up ## [1] 26168.72 dry_year = dry_day/365 dry_year # year aquifer dries up ## [1] 71.69513 # position at nov 1 1994 dry_year + 1994.917 ## [1] 2066.612 Any questions? 9.2 Confidence intervals for derived quantities Point estimates aren’t useful to statisticians Show me the intervals beta_ci = confint(m1) beta_ci[1,] # ci for b0 ## 2.5 % 97.5 % ## 95.34554 99.16611 beta_ci[2,] # ci for b1 ## 2.5 % 97.5 % ## -0.004181951 -0.003251031 # straight to jail! dry_day_ci = -beta_ci[1,]/beta_ci[2,] dry_day_ci ## 2.5 % 97.5 % ## 22799.30 30502.97 1994.917 + (dry_day_ci/365) ## 2.5 % 97.5 % ## 2057.381 2078.487 Consider the random variable \\(X\\). \\(X\\) is a sequence of random values controlled by a distribution. The mean of \\(X\\) is constant upon realizing the values of \\(x\\). Consider the function \\(g()\\). \\(g()\\) is a differentiable function, say \\(g(y) = y^2\\). Consider \\(g(X)\\). \\(g(X)\\) is not just the values of \\(X\\) squared. \\(X\\) is controlled by a distribution function. To square a function is not always equivalent to squaring its output. This is why \\(se\\left[ g(X) \\right] \\neq g(se(X))\\). Delta method For a random variable \\(X\\) and differentiable function \\(g()\\), the covariance of \\(g(X)\\) can be approximated as: \\[ Cov(g(X)) = g^\\prime(\\mu)Cov(X)\\left[ g^\\prime(\\mu)\\right]^T \\] Done “manually” x1 = as.numeric(coef(m1)[1]) # b0 x2 = as.numeric(coef(m1)[2]) # b1 # first derivative of g(X) = -x1/x2 gprime_mu = attr(eval(deriv(~ -x1/x2, c(&quot;x1&quot;,&quot;x2&quot;))), &quot;gradient&quot;) # approximate covariance of g(X) cov_g = gprime_mu%*%vcov(m1)%*%t(gprime_mu) sqrt(cov_g) ## [,1] ## [1,] 1447.542 # wald-type confidence intervals dry_day_l = dry_day - 1.96 * sqrt(cov_g) dry_day_u = dry_day + 1.96 * sqrt(cov_g) # lower estimate 1994.917 + dry_day_l/365 ## [,1] ## [1,] 2058.839 # upper estimate 1994.917 + dry_day_u/365 ## [,1] ## [1,] 2074.385 Done with the msm package library(msm) cov_g2 = deltamethod(~ -x1/x2, mean = coef(m1), cov = vcov(m1)) cov_g2 ## [1] 1447.542 1994.917 + ((dry_day - 1.96*cov_g2)/365) ## [1] 2058.839 1994.917 + ((dry_day + 1.96*cov_g2)/365) ## [1] 2074.385 9.3 Non-parametric bootstrap For a dataset with \\(n\\) observations, take a sample of size \\(n\\) with replacement. Fit the sampled data to the chosen statistical model. Save the parameters and/or estiamtes of interest. Repeat the process \\(m\\) times. Pseudo-code SET \\(m \\to m\\) SET \\(n \\to \\text{LENGTH}(Y)\\) FOR \\(i = 1\\) TO \\(m\\) DO   DRAW \\((S_1, S_2, \\dots, S_n)\\) independently from \\(\\{1,2,\\dots,n\\}\\)   SET \\(\\mathbf{S} = (S_1, S_2, \\dots, S_n)\\)   DEFINE \\((x_j^*, y_j^*) = (x_{S_j}, y_{S_j})\\) for \\(j = 1,2,\\dots,n\\)   FIT \\(y_j^* = \\beta_0^* + \\beta_1^* x_j^* + \\varepsilon_j^*\\) for \\(j = 1,2,\\dots,n\\)   COMPUTE \\(\\theta^* = \\frac{-\\beta_0^*}{\\beta_1^*}\\)   SET \\(\\theta_i \\to \\mathbf{\\theta}^*\\)   SET \\(i \\to i + 1\\) END FOR R code # reproducibility seed # if we change this then the results will change with it set.seed(73) # number of bootstrap iterations m = 1000 # number of observations in the data n = nrow(wlev) # empty matrix for derived quantities dry_day_boot = matrix(,m,1) # for loop across bootstrap iterations for(i in 1:m){ # sample N times from the data WITH replacement boot_sample = sample(1:n, replace = TRUE) # create temporary data for that iteration of sampling temp_data = wlev[boot_sample,c(19,21)] # fit the model to the sampled data model = lm(lev_va_ft ~ day_i, data = temp_data) # compute dry day using the coefficients of that model dry_day_boot[i,] = -coef(model)[1]/coef(model)[2] } Inference from confidence intervals What does a confidence intervals actually measure? Sampling distributions Hypothetical, unobservable mathematical objects Confidence intervals are derived from the assumption of their existence Empirical distributions Most STAT 225 students could tell you how to work with these Summary statistics and plots are intuitive Treat the empirical distribution like the sampling distribution library(latex2exp) hist(dry_day_boot,col=&quot;white&quot;,xlab=&quot;Day&quot;, main=TeX( &#39;Empirical distribuiton of $$-$$\\\\hat{\\\\frac{$\\\\beta_0}{$\\\\beta_1}}&#39;), freq=FALSE, breaks=20) Affine transformation The original information is preserved Except for the meaning of \\(0\\) dry_year_boot = (dry_day_boot/365) + 1994.917 hist(dry_year_boot,col=&quot;white&quot;,xlab=&quot;Year&quot;, main=TeX( &#39;Empirical distribuiton of $$-$$\\\\hat{\\\\frac{$\\\\beta_0}{$\\\\beta_1}}&#39;), freq=FALSE,breaks=20) # five number summary of empirical distribution of # estimated year that the ogallala dries up quantile(dry_year_boot, c(0,0.25,0.5,0.75,1)) ## 0% 25% 50% 75% 100% ## 2056.961 2064.169 2066.813 2069.727 2090.409 # 95% CI based on empirical distribution quantile(dry_year_boot, c(0.025, 0.975)) ## 2.5% 97.5% ## 2060.030 2076.428 The value of vectorization # bootstrap iterations m = 1000 # x and y data x = wlev$day_i y = wlev$lev_va_ft # number of observations in the data n = length(y) # reproducibility seed set.seed(73) # replicate acts similarly to a for loop dry_day_boot2 = replicate(m, { # sampling index idx = sample.int(n, n, replace = TRUE) # sample from x and y x_boot = x[idx] y_boot = y[idx] # compute the mean of each sample xbar = mean(x_boot) ybar = mean(y_boot) # perform scalar form simple linear regression for the sample b1 = sum((x_boot - xbar) * (y_boot - ybar)) / sum((x_boot - xbar)^2) b0 = ybar - b1 * xbar # compute dry day -b0 / b1 }) Same algorithm, just sped up a lot The computation time was reduced by an order of magnitude Note: this works because we’re using simple linear regression dry_year_boot2 = (dry_day_boot2/365) + 1994.917 quantile(dry_year_boot2, c(0,0.25,0.5,0.75,1)) ## 0% 25% 50% 75% 100% ## 2056.961 2064.169 2066.813 2069.727 2090.409 quantile(dry_year_boot, c(0.025,0.975)) ## 2.5% 97.5% ## 2060.030 2076.428 Packages My philosophy: it’s better to avoid packages until you understand what they’re doing in full a.k.a. do it in base R before you solve your problem with a package Then it’s just a time save (i.e., I don’t program my machine learning models from scratch every time… but I can) You are always at the mercy of the package authors optimization skills If you have a bad computer, it’s sometimes more efficient to just become a better programmer library(mosaic) # one of many package options for bootstrapping # reproducibility seed set.seed(73) # mosaic&#39;s bootstrap bootstrap = do(1000) * coef(lm(lev_va_ft ~ day_i, data = resample(wlev))) # fills a dataframe* with each parameter from each iteration # *check str(bootstrap) and tell me why im lying head(bootstrap) ## Intercept day_i ## 1 97.06151 -0.003888550 ## 2 97.23733 -0.003629539 ## 3 97.86672 -0.003786088 ## 4 95.07953 -0.003089260 ## 5 96.99797 -0.003701108 ## 6 97.02268 -0.003890343 # compute the dry day theta_hat = -bootstrap[,1]/bootstrap[,2] quantile(theta_hat, c(0.025,0.975)) ## 2.5% 97.5% ## 23766.07 29751.60 quantile(((theta_hat/365) + 1994.917), c(0.025,0.975)) ## 2.5% 97.5% ## 2060.030 2076.428 9.4 Concept demo Bootstrapping shiny app 9.5 R programming Lab Read these two pages from [Wood (2006) # install.packages(&quot;gamair&quot;) library(gamair) data(hubble) par(mar = c(4.5,4.5,2,1)) plot(hubble$x,hubble$y, xlab = &quot;Distance in Mega parsecs&quot;, ylab = &quot;Velocity (km/s)&quot;, pch = 19) m = lm(hubble$y ~ hubble$x - 1) abline(m) Using the hubble data in the gamair package: Write out a linear model for predicting the velocity of the Cepheid stars observed by the Hubble space telescope based on their distance in mega parsecs, based on Hubble’s law. State and briefly explain what the assumptions behind that model are. Fit the data in R using the matrix form of method of least squares. Save the results to a variable in R. \\[ \\boldsymbol{\\beta} = (\\boldsymbol{X}^\\prime \\boldsymbol{X})^{-1} \\boldsymbol{X}^\\prime y \\] Fit the data in R using the lm() function. Save the model. Compare the coefficients from (3) and (4). Are they different? Add the fitted regression line from (4) to the plot of the original data. Set the line width to 2 and change the color to be different from the points. Compute the confidence interval for the coefficients in (4) using any method you prefer. Save them. Based off of the reading, predict the age of the universe. Note that the distance is measured in Mega parsecs, which are \\(3.09 \\times 10^19\\) km. Using the confidence interval from (7), estimate the lower and upper bound for the age of the universe. Estimate the lower and upper bound using the Delta Method instead. Estimate the lower and upper bound using non-parametric bootstrapping. Compare the three confidence intervals. Which of the three is widest? Which of the three is valid? 9.6 Go away 9.7 Extended notes on the Delta Method Delta method To appropriately comprehend the rationale behind the Delta method it is important to recognize that it arises from the combination of four theorems: Taylor’s theorem (as a by-product, the mean value theorem) The Strong Central Limit theorem The Continuous Mapping theorem Slutsky’s theorem As statistics is founded in the mathematics of probability theory, we should not be alarmed to see that our fundamental theorems are expressed using real analytics. For a detailed proof of the Central Limit theorem and Delta Method you could do worse by reading pages 236 - 243 of Statistical Inference \\(2^{nd}\\) edition, by George Casella and Roger L. Berger. Below I’ve provided the statements of each theorem relevant to the Delta Method. Anyone with interest in understanding the mathematics behind statistics should take a moment to review these and try to piece together how they might produce the final result of the Delta Method. Extending this exercise to instead deriving a proof of the Delta Method isn’t a terrible idea for anyone considering a career in the quantitative sciences. Taylor polynomial: If a function \\(g(x)\\) has derivatives of order \\(r\\), that is, \\(g^{(r)}(x) = \\frac{d^r}{dx^r}\\) exists, then for any constant \\(a\\), the Taylor polynomial of order \\(r\\) about \\(a\\) is \\[ T_r(x) = \\sum_{i=0}^r \\frac{g^{(i)}(a)}{i!}(x-a)^i. \\] The Central Limit Theorem: for a random variable \\(X\\), \\(\\bar{X}_n = \\frac{1}{n}X_i\\) has the limiting distribution \\(N(\\mu, \\sigma^2 n^{-1})\\) \\[ \\begin{aligned} &amp; \\bar{X}_n \\xrightarrow{d} N(\\mu, \\sigma^2 n^{-1}), \\\\ \\\\ &amp; \\bar{X}_n - \\mu \\xrightarrow{d} N(0, \\sigma^2 n^{-1}), \\\\ \\\\ &amp; \\sqrt{n}(\\bar{X}_n - \\mu) \\xrightarrow{d} N(0, \\sigma^2), \\\\ \\\\ &amp; \\sqrt{n}(\\bar{X}_n - \\mu)/\\sigma \\xrightarrow{d} N(0, 1). \\\\ \\end{aligned} \\] Continuous Mapping Theorem: A function that satisfies \\(x_n \\rightarrow x \\text{ and } g(x_n) \\rightarrow g(x)\\) for any deterministic sequence \\(x_n\\) will satisfy then same convergence for any sequence of random variables \\(X_n\\). Slutsky’s Theorem: If \\(X_n \\rightarrow X\\) in distribution and \\(Y_n \\rightarrow a\\), a constant, in probability, then \\[ \\begin{aligned} Y_n X_n \\rightarrow aX \\text{ in distribution.}\\\\ \\\\ X_n + Y_n \\rightarrow X + a \\text{ in distribution.} \\end{aligned} \\] Delta method: Let \\(Y_n\\) be a sequence of random variables that satisfied \\(\\sqrt{n}(Y_n - \\theta) \\xrightarrow{d} N(0,\\sigma^2)\\). For a given function \\(g\\) and a specific value of \\(\\theta\\), suppose that \\(g^\\prime(\\theta)\\) exists and is not \\(0\\). Then \\[ \\sqrt{n}\\left[g(Y_n) - g(\\theta) \\right] \\xrightarrow{d} N(0, \\sigma^2 \\left[ g^\\prime(\\theta) \\right]^2). \\] "],["day-10.html", "10 Day 10 10.1 Review 10.2 In-class activity 10.3 Assignment 3 10.4 Projects 10.5 Class choice lectures 10.6 Go away", " 10 Day 10 10.1 Review Confidence intervals for derived quantities The wrong way beta_ci = confint(m1) beta_ci[1,] # ci for b0 ## 2.5 % 97.5 % ## 95.34554 99.16611 beta_ci[2,] # ci for b1 ## 2.5 % 97.5 % ## -0.004181951 -0.003251031 # straight to jail! dry_day_ci = -beta_ci[1,]/beta_ci[2,] dry_day_ci ## 2.5 % 97.5 % ## 22799.30 30502.97 1994.917 + (dry_day_ci/365) ## 2.5 % 97.5 % ## 2057.381 2078.487 The delta method “Manually” x1 = as.numeric(coef(m1)[1]) # b0 x2 = as.numeric(coef(m1)[2]) # b1 # first derivative of g(X) = -x1/x2 gprime_mu = attr(eval(deriv(~ -x1/x2, c(&quot;x1&quot;,&quot;x2&quot;))), &quot;gradient&quot;) # approximate covariance of g(X) cov_g = gprime_mu%*%vcov(m1)%*%t(gprime_mu) sqrt(cov_g) ## [,1] ## [1,] 1447.542 # wald-type confidence intervals dry_day_l = dry_day - 1.96 * sqrt(cov_g) dry_day_u = dry_day + 1.96 * sqrt(cov_g) # lower estimate 1994.917 + dry_day_l/365 ## [,1] ## [1,] 2058.839 # upper estimate 1994.917 + dry_day_u/365 ## [,1] ## [1,] 2074.385 msm package library(msm) cov_g2 = deltamethod(~ -x1/x2, mean = coef(m1), cov = vcov(m1)) cov_g2 ## [1] 1447.542 1994.917 + ((dry_day - 1.96*cov_g2)/365) ## [1] 2058.839 1994.917 + ((dry_day + 1.96*cov_g2)/365) ## [1] 2074.385 Non-parametric bootstrap For a dataset with \\(n\\) observations, take a sample of size \\(n\\) with replacement. Fit the sampled data to the chosen statistical model. Save the parameters and/or estiamtes of interest. Repeat the process \\(m\\) times. Pseudo-code SET \\(m \\to m\\) SET \\(n \\to \\text{LENGTH}(Y)\\) FOR \\(i = 1\\) TO \\(m\\) DO   DRAW \\((S_1, S_2, \\dots, S_n)\\) independently from \\(\\{1,2,\\dots,n\\}\\)   SET \\(\\mathbf{S} = (S_1, S_2, \\dots, S_n)\\)   DEFINE \\((x_j^*, y_j^*) = (x_{S_j}, y_{S_j})\\) for \\(j = 1,2,\\dots,n\\)   FIT \\(y_j^* = \\beta_0^* + \\beta_1^* x_j^* + \\varepsilon_j^*\\) for \\(j = 1,2,\\dots,n\\)   COMPUTE \\(\\theta^* = \\frac{-\\beta_0^*}{\\beta_1^*}\\)   SET \\(\\theta_i \\to \\mathbf{\\theta}^*\\)   SET \\(i \\to i + 1\\) END FOR R code # reproducibility seed # if we change this then the results will change with it set.seed(73) # number of bootstrap iterations m = 1000 # number of observations in the data n = nrow(wlev) # empty matrix for derived quantities dry_day_boot = matrix(,m,1) # for loop across bootstrap iterations for(i in 1:m){ # sample N times from the data WITH replacement boot_sample = sample(1:n, replace = TRUE) # create temporary data for that iteration of sampling temp_data = wlev[boot_sample,c(19,21)] # fit the model to the sampled data model = lm(lev_va_ft ~ day_i, data = temp_data) # compute dry day using the coefficients of that model dry_day_boot[i,] = -coef(model)[1]/coef(model)[2] } Inference from confidence intervals What does a confidence intervals actually measure? Sampling distributions Hypothetical, unobservable mathematical objects Confidence intervals are derived from the assumption of their existence Empirical distributions Summary statistics and plots are intuitive Empirical distribution \\(\\approx\\) sampling distribution library(latex2exp) hist(dry_day_boot,col=&quot;white&quot;,xlab=&quot;Day&quot;, main=TeX( &#39;Empirical distribuiton of $$-$$\\\\hat{\\\\frac{$\\\\beta_0}{$\\\\beta_1}}&#39;), freq=FALSE, breaks=20) Affine transformation dry_year_boot = (dry_day_boot/365) + 1994.917 hist(dry_year_boot,col=&quot;white&quot;,xlab=&quot;Year&quot;, main=TeX( &#39;Empirical distribuiton of $$-$$\\\\hat{\\\\frac{$\\\\beta_0}{$\\\\beta_1}}&#39;), freq=FALSE,breaks=20) # five number summary of empirical distribution of # estimated year that the ogallala dries up quantile(dry_year_boot, c(0,0.25,0.5,0.75,1)) ## 0% 25% 50% 75% 100% ## 2056.961 2064.169 2066.813 2069.727 2090.409 # 95% CI based on empirical distribution quantile(dry_year_boot, c(0.025, 0.975)) ## 2.5% 97.5% ## 2060.030 2076.428 Optimized bootstrap for simple linear regression # bootstrap iterations m = 1000 # x and y data x = wlev$day_i y = wlev$lev_va_ft # number of observations in the data n = length(y) # reproducibility seed set.seed(73) # replicate acts similarly to a for loop dry_day_boot2 = replicate(m, { # sampling index idx = sample.int(n, n, replace = TRUE) # sample from x and y x_boot = x[idx] y_boot = y[idx] # compute the mean of each sample xbar = mean(x_boot) ybar = mean(y_boot) # perform scalar form simple linear regression for the sample b1 = sum((x_boot - xbar) * (y_boot - ybar)) / sum((x_boot - xbar)^2) b0 = ybar - b1 * xbar # compute dry day -b0 / b1 }) dry_year_boot2 = (dry_day_boot2/365) + 1994.917 quantile(dry_year_boot2, c(0,0.25,0.5,0.75,1)) ## 0% 25% 50% 75% 100% ## 2056.961 2064.169 2066.813 2069.727 2090.409 quantile(dry_year_boot, c(0.025,0.975)) ## 2.5% 97.5% ## 2060.030 2076.428 Using the mosaic package library(mosaic) # one of many package options for bootstrapping # reproducibility seed set.seed(73) # mosaic&#39;s bootstrap bootstrap = do(1000) * coef(lm(lev_va_ft ~ day_i, data = resample(wlev))) # fills a dataframe* with each parameter from each iteration # *check str(bootstrap) and tell me why im lying head(bootstrap) ## Intercept day_i ## 1 97.06151 -0.003888550 ## 2 97.23733 -0.003629539 ## 3 97.86672 -0.003786088 ## 4 95.07953 -0.003089260 ## 5 96.99797 -0.003701108 ## 6 97.02268 -0.003890343 # compute the dry day theta_hat = -bootstrap[,1]/bootstrap[,2] quantile(theta_hat, c(0.025,0.975)) ## 2.5% 97.5% ## 23766.07 29751.60 quantile(((theta_hat/365) + 1994.917), c(0.025,0.975)) ## 2.5% 97.5% ## 2060.030 2076.428 Any questions? 10.2 In-class activity Work together with your partner to find data from Dryad, Kaggle, or another valid data repository. Select at least \\(3\\) and at most \\(6\\) variables from the data. Compute the mean, standard deviation, and five number summary for those variables. Create at least 2 different graphics from those variables, describing their distributions or relationships with one another. Select a response variable, then write out a linear model for predicting that response. Fit the model to the data using the lm() function in R. Produce a summary, coefficients, and confidence intervals for that model. Plot the fitted regression line for the model onto a scatterplot for each variable with the response. 10.3 Assignment 3 hilarious 10.4 Projects Week 8 is closer than you think Start thinking about what you want to do Start thinking about who you might work well with Think harder about who you won’t work well with There’s no shame in being incompatible in a group Try to align your strengths In a statistics classroom, I would pair myself with someone more mathematically inclined. In a computer science classroom, I’m the math guy. In a biology classroom, I’m whatever my project team sucks the most at. A note to provide zero pressure to the project whatsoever Publications are very valuable in hiring Kansas Water Institute, ID3A, etc. 10.5 Class choice lectures Get curious (please) Statistical topics are like wizard names Longer name = chill, relaxed, someone you’d get a beer with Shorter name = fear for your life If the class has no direction/curiosity we’re doing: GLMMs Bayesian models Eigenvalues Search Those lectures will not be pretty There will be math This is a threat (I’m joking) (Legally speaking) 10.6 Go away "],["assignment-1-2.html", "11 Assignment 1", " 11 Assignment 1 The goal of this assignment is to introduce you to R, RStudio, and R markdown. We will be using these three programs throughout the course. You will be using these programs to make assignments (and other documents) that involve reporting the results obtained from R easily reproducible. In the most basic sense, this is equivalent to “showing your work” as you would for assignments with analytical components. As I mentioned in class, using R, RStudio, and R markdown is not required for this class. You are free to use whatever programming language you are familiar with as long as it is capable of doing basic statistical functions and making documents that enable your work to be reproducible. If you do not plan on using R, RStudio, and R markdown, please send me an email that explains what software you are using and why you want to use it. I do reserve the right to deny the use of certain software if I think it is incapable of doing what is required for this class. If you choose not to use R, RStudio, and R markdown, you must complete the assignment using the software of your choice. Install R and RStudio. In RStudio, create a new R Markdown file. Change the default output format to PDF. Within the new R markdown file, delete the existing code and text (be carful to retain the top four lines which R markdown requires). Using the code below, download the annual \\(\\text{CO}_2\\) concentrations from Mt. Loa. With the code provided, plot the data. url &lt;- &quot;https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_annmean_mlo.txt&quot; df &lt;- read.table(url,header=FALSE) colnames(df) &lt;- c(&quot;year&quot;,&quot;meanCO2&quot;,&quot;unc&quot;) plot(x = df$year, y = df$meanCO2, xlab = &quot;Year&quot;, ylab = expression(&quot;Mean annual &quot;*CO[2]*&quot; concentration&quot;), main=&quot;your name here&quot;,col=&quot;blue&quot;) Change the plot title from “your name here” to your actual name. Change the color of the points from blue to a color of your choice. Knit the file to HTML or pdf and save the file as Yourlastname_Assignment1 (e.g., Sholl_Assignment1). Upload this file to Canvas by 11:59 pm on Friday 01/23/26. "],["assignment-2.html", "12 Assignment 2", " 12 Assignment 2 In an R Markdown, complete problems 1-12 showing all relevant code and output. Please be sure to comment all non-redundant lines of code or fully explain each code block in the documentation itself. Title the document with you and your partner’s last names. Name the document Lastname1_Lastname2_Assignment2 (Sholl_McKay_Assignment2.pdf) and upload it to Canvas by 11:59 pm on Friday 02/07/26. Only one submission is needed per group. Set the seed to “73” (set.seed(73)). Define 3 scalars: \\[ a = 3 \\qquad\\qquad b = 5 \\qquad\\qquad c = 0.25 \\] Define 3 vectors: \\[ \\boldsymbol{x} = \\begin{bmatrix} 5 \\\\ 3 \\\\ 6 \\end{bmatrix} \\qquad\\qquad \\boldsymbol{y} = \\begin{bmatrix} 11.3 \\\\ 8.97 \\\\ 9.82 \\end{bmatrix} \\qquad\\qquad \\boldsymbol{z} = \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix} \\] Generate 3 standard normal values and save them to a variable named w. Multiply w by a random uniform value bound between 1 and 5, then save it as the new w. Define the matrix: \\[ \\textbf{A} = \\begin{bmatrix} \\boldsymbol{x} + w_1 \\\\ \\boldsymbol{y} + w_2 \\\\ \\boldsymbol{z} + w_3 \\end{bmatrix} \\] Confirm the following is true: \\[ \\textbf{AA}^{-1} = \\textbf{I} \\] Add a row of 3 gamma distributed values with shape equal to 2 and rate equal to 1/5. Simulate 4 normally distributed data points with a mean equal to the trace of A and standard deviation of 5. Save them as a variable named q. Add a column of 1s to the matrix A and define: \\[ \\hat{\\boldsymbol{\\theta}} = (\\boldsymbol{A}^\\prime \\boldsymbol{A})^{-1} \\boldsymbol{A}^\\prime \\boldsymbol{q} \\] Calculate the predicted values of q: \\[ \\hat{\\boldsymbol{q}} = \\boldsymbol{A} \\boldsymbol{\\hat{\\theta}} \\] Calculate the confidence interval for the mean of \\(\\boldsymbol{q}\\) and \\(\\hat{\\boldsymbol{q}}\\). Simulate 1000 normally distributed data points with the same mean and standard deviation as in question (8). Plot them on a histogram. Include vertical lines for the lower and upper bound on the mean of \\(\\boldsymbol{q}\\). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
